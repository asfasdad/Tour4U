# -*- coding: utf-8 -*-
"""
ç«žå“åˆ†æžå·¥å…· v15 â€” æ ¸å¿ƒæž¶æž„é‡æž„ç‰ˆ
- å¤šè½®ä¼šè¯é“¾ (Session + HistoryItem)
- ç”Ÿäº§è€…-æ¶ˆè´¹è€…çˆ¬è™« + å®žæ—¶çœ‹æ¿
- è‡ªåŠ¨æŒä¹…åŒ– & å¹¿å‘Šç»„å¯¹æ¯”
- å¼‚æ­¥æŠ¥å‘Šç”Ÿæˆ
"""
import streamlit as st
import pandas as pd
from DrissionPage import ChromiumPage, ChromiumOptions
from urllib.parse import urlparse
import sys
import time
import random
import requests
import threading
import queue
import csv
import io
from datetime import datetime
import os
import re
import sqlite3
import hashlib
import json
import altair as alt
import socket
import subprocess

from llm_providers import get_provider, PLATFORM_MODELS, DEFAULT_BASE_URLS
from session_store import (
    get_session_for_file,
    save_session_for_file,
    list_history_files_with_sessions,
)
from data_diff import compute_diff, make_snapshot, get_diff_summary_for_ui, sku_fingerprint, infer_price_from_text

# --- å¸¸é‡ï¼ˆå†…ç½®é…ç½®ï¼Œä¸æš´éœ²åœ¨ UIï¼‰---
if getattr(sys, "frozen", False):
    APP_BASE_DIR = os.path.dirname(sys.executable)
else:
    APP_BASE_DIR = os.path.dirname(os.path.abspath(__file__))

HISTORY_DIR = os.path.join(APP_BASE_DIR, "history_data")
DEFAULT_TARGET_URL = "https://www.google.com"
if not os.path.exists(HISTORY_DIR):
    os.makedirs(HISTORY_DIR)

DB_PATH = os.path.join(HISTORY_DIR, "analysis.sqlite3")
DB_LOCK = threading.Lock()


@st.cache_resource
def _get_engine_resources():
    lock = threading.Lock()
    state = {
        "running": False,
        "stop_requested": False,
        "error": None,
        "done": 0,
        "fail": 0,
        "pending": 0,
        "discovered": 0,
        "save_filename": "",
        "shared_list": None,
        "transferred": False,
    }
    return lock, state


ENGINE_LOCK, ENGINE_STATE = _get_engine_resources()


def _url_md5(url: str) -> str:
    return hashlib.md5((url or "").encode("utf-8")).hexdigest()


def _get_query_param(url: str, keys: list[str]) -> str:
    try:
        from urllib.parse import parse_qs, urlparse

        qs = parse_qs(urlparse(url).query)
        for key in keys:
            for k, v in qs.items():
                if k.lower() == key.lower() and v:
                    return str(v[0]).strip()
    except Exception:
        pass
    return ""


def _extract_campaign_id_from_url(url: str) -> str:
    return _get_query_param(url, ["campaignid", "gad_campaignid", "gbraid"])


def _extract_adgroup_id_from_url(url: str) -> str:
    return _get_query_param(url, ["adgroupid", "adgroup_id", "adset_id", "adsetid"])


def _find_free_port(preferred_range: tuple[int, int] = (20000, 40000)) -> int:
    start, end = preferred_range
    for _ in range(60):
        port = random.randint(start, end)
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
            try:
                s.bind(("127.0.0.1", port))
                return port
            except OSError:
                continue
    # fallback: let OS pick
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        s.bind(("127.0.0.1", 0))
        return int(s.getsockname()[1])


def _engine_thread_run(k_list: list[str], settings: dict, current_save_path: str, current_save_filename: str) -> None:
    """åŽå°çº¿ç¨‹è¿è¡Œå¼•æ“Žã€‚ä¸è¦åœ¨æ­¤å‡½æ•°é‡Œè°ƒç”¨ä»»ä½• st.* APIã€‚"""
    try:
        with ENGINE_LOCK:
            ENGINE_STATE.update(
                {
                    "running": True,
                    "stop_requested": False,
                    "error": None,
                    "done": 0,
                    "fail": 0,
                    "pending": 0,
                    "discovered": 0,
                    "save_filename": current_save_filename,
                    "shared_list": [],
                    "transferred": False,
                }
            )

        co = ChromiumOptions()
        co.set_argument("--no-first-run")
        # profile
        try:
            profile_dir = os.path.join(HISTORY_DIR, "chrome_profiles", settings.get("batch_id", "run"))
            os.makedirs(profile_dir, exist_ok=True)
            co.set_argument(f"--user-data-dir={profile_dir}")
        except Exception:
            pass
        # local port
        try:
            co.set_local_port(_find_free_port((20000, 40000)))
        except Exception:
            pass
        if settings.get("force_headless"):
            co.set_argument("--headless=new")
        else:
            co.headless(False)
        co.mute(True)
        if settings.get("proxy"):
            co.set_proxy(settings.get("proxy"))

        # browser
        try:
            browser = ChromiumPage(co)
        except Exception as e:
            # proxy fallback
            if settings.get("proxy") and "Handshake status 404" in str(e):
                co2 = ChromiumOptions()
                co2.set_argument("--no-first-run")
                try:
                    profile_dir = os.path.join(HISTORY_DIR, "chrome_profiles", settings.get("batch_id", "run") + "_noproxy")
                    os.makedirs(profile_dir, exist_ok=True)
                    co2.set_argument(f"--user-data-dir={profile_dir}")
                except Exception:
                    pass
                try:
                    co2.set_local_port(_find_free_port((20000, 40000)))
                except Exception:
                    pass
                if settings.get("force_headless"):
                    co2.set_argument("--headless=new")
                else:
                    co2.headless(False)
                co2.mute(True)
                browser = ChromiumPage(co2)
            else:
                raise

        link_queue = queue.Queue()
        total_discovered = [0]
        lock = threading.Lock()
        counters = {"done": 0, "fail": 0}
        is_running_ref = [True]

        # producer in this thread (DrissionPage must be single-thread; background is ok)
        # NOTE: we cannot use st.session_state here; use ENGINE_STATE.stop_requested
        base = DEFAULT_TARGET_URL.rstrip("/")
        for keyword in k_list:
            with ENGINE_LOCK:
                if ENGINE_STATE.get("stop_requested"):
                    break
            tab = None
            try:
                tab = browser.new_tab()
                gl, hl = settings.get("gl", "us"), settings.get("hl", "en")
                for i in range(int(settings.get("pages", 1))):
                    with ENGINE_LOCK:
                        if ENGINE_STATE.get("stop_requested"):
                            break
                    search_url = f"{base}/search?q={keyword}&start={i*10}&gl={gl}&hl={hl}&pws=0"
                    try:
                        tab.get(search_url, timeout=25)
                        time.sleep(2)
                        tab.scroll.to_bottom()
                        time.sleep(1)
                        tab.scroll.to_top()
                        time.sleep(0.5)
                    except Exception:
                        pass
                    try:
                        links = tab.eles("xpath://a[@href]")
                        for link in links:
                            with ENGINE_LOCK:
                                if ENGINE_STATE.get("stop_requested"):
                                    break
                            try:
                                href = link.attr("href")
                                if href and "/aclk" in href:
                                    link_queue.put((keyword, href))
                                    total_discovered[0] += 1
                            except Exception:
                                continue
                    except Exception:
                        pass
                    with ENGINE_LOCK:
                        ENGINE_STATE["discovered"] = total_discovered[0]
                        ENGINE_STATE["pending"] = link_queue.qsize()
                    time.sleep(random.uniform(1, 2))
            finally:
                if tab:
                    try:
                        tab.close()
                    except Exception:
                        pass

        # end signals
        for _ in range(int(settings.get("num_workers", 2))):
            link_queue.put(None)

        # consumers
        workers = []
        for _ in range(int(settings.get("num_workers", 2))):
            t = threading.Thread(
                target=consumer_worker,
                args=(link_queue, ENGINE_STATE["shared_list"], lock, counters, is_running_ref, settings, current_save_path),
                daemon=True,
            )
            t.start()
            workers.append(t)

        while True:
            stop_now = False
            with ENGINE_LOCK:
                if ENGINE_STATE.get("stop_requested"):
                    is_running_ref[0] = False
                    stop_now = True
            if stop_now:
                # ç«‹å³æ¸…ç©ºé˜Ÿåˆ—ï¼Œé¿å…å¡åœ¨å¤§é‡ pending
                try:
                    while True:
                        link_queue.get_nowait()
                except Exception:
                    pass
                for _ in range(int(settings.get("num_workers", 2))):
                    try:
                        link_queue.put_nowait(None)
                    except Exception:
                        pass
                # ç­‰å¾…ä¸€å°æ®µæ—¶é—´è®©æ¶ˆè´¹è€…é€€å‡º
                for t in workers:
                    try:
                        t.join(timeout=3)
                    except Exception:
                        pass
                break

            with ENGINE_LOCK:
                ENGINE_STATE["done"] = counters["done"]
                ENGINE_STATE["fail"] = counters["fail"]
                ENGINE_STATE["pending"] = link_queue.qsize()
                ENGINE_STATE["discovered"] = total_discovered[0]
            if link_queue.qsize() == 0 and not any(t.is_alive() for t in workers):
                break
            time.sleep(0.4)

        try:
            browser.quit()
        except Exception:
            pass

    except Exception as e:
        with ENGINE_LOCK:
            ENGINE_STATE["error"] = str(e)
    finally:
        with ENGINE_LOCK:
            ENGINE_STATE["running"] = False


def _get_listening_pids_on_port(port: int) -> list[int]:
    """Windows: parse netstat to find LISTENING PIDs on a port."""
    try:
        out = subprocess.check_output(["netstat", "-ano"], text=True, errors="ignore")
    except Exception:
        return []
    pids: set[int] = set()
    needle = f":{int(port)}"
    for line in out.splitlines():
        line = line.strip()
        if not line:
            continue
        if needle not in line:
            continue
        if "LISTENING" not in line.upper():
            continue
        parts = line.split()
        if len(parts) >= 5:
            try:
                pids.add(int(parts[-1]))
            except Exception:
                pass
    return sorted(pids)


def _db_connect() -> sqlite3.Connection:
    conn = sqlite3.connect(DB_PATH, timeout=30)
    conn.execute("PRAGMA journal_mode=WAL;")
    conn.execute("PRAGMA foreign_keys=ON;")
    return conn


def _table_columns(conn: sqlite3.Connection, table: str) -> set[str]:
    rows = conn.execute(f"PRAGMA table_info({table});").fetchall()
    return {r[1] for r in rows}


def init_sqlite_schema() -> None:
    """åˆ›å»º/æ›´æ–°ä¸‰å¼ è¡¨ï¼šad_impressions, products, product_statesã€‚"""
    with DB_LOCK:
        conn = _db_connect()
        try:
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS ad_impressions (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    observed_at TEXT,
                    keyword TEXT,
                    gad_campaignid TEXT,
                    url TEXT,
                    raw_url TEXT,
                    domain TEXT,
                    brand TEXT,
                    ad_type TEXT,
                    ad_signals TEXT,
                    product_id TEXT,
                    batch_id TEXT,
                    campaign TEXT
                );
                """
            )
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS products (
                    id TEXT PRIMARY KEY,
                    domain TEXT,
                    title TEXT,
                    handle TEXT,
                    image_url TEXT,
                    first_seen_at TEXT
                );
                """
            )
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS product_states (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    product_id TEXT,
                    observed_at TEXT,
                    price REAL,
                    compare_at_price REAL,
                    is_available INTEGER,
                    currency TEXT,
                    FOREIGN KEY(product_id) REFERENCES products(id)
                );
                """
            )

            # è½»é‡â€œå­˜åœ¨åˆ™æ›´æ–°â€ï¼šè¡¥é½å¯èƒ½ç¼ºå¤±çš„åˆ—
            cols = _table_columns(conn, "ad_impressions")
            for col, ddl in {
                "raw_url": "ALTER TABLE ad_impressions ADD COLUMN raw_url TEXT;",
                "product_id": "ALTER TABLE ad_impressions ADD COLUMN product_id TEXT;",
                "batch_id": "ALTER TABLE ad_impressions ADD COLUMN batch_id TEXT;",
                "campaign": "ALTER TABLE ad_impressions ADD COLUMN campaign TEXT;",
            }.items():
                if col not in cols:
                    conn.execute(ddl)

            cols2 = _table_columns(conn, "product_states")
            for col, ddl in {
                "compare_at_price": "ALTER TABLE product_states ADD COLUMN compare_at_price REAL;",
                "currency": "ALTER TABLE product_states ADD COLUMN currency TEXT;",
            }.items():
                if col not in cols2:
                    conn.execute(ddl)

            cols3 = _table_columns(conn, "products")
            if "image_url" not in cols3:
                conn.execute("ALTER TABLE products ADD COLUMN image_url TEXT;")

            conn.commit()
        finally:
            conn.close()


def load_sqlite_table_as_df(query: str) -> pd.DataFrame:
    with DB_LOCK:
        conn = _db_connect()
        try:
            return pd.read_sql_query(query, conn)
        finally:
            conn.close()


def sql_df(query: str, params: tuple | list | None = None) -> pd.DataFrame:
    with DB_LOCK:
        conn = _db_connect()
        try:
            return pd.read_sql_query(query, conn, params=params)
        finally:
            conn.close()


def get_selected_domain() -> str:
    return str(st.session_state.get("selected_domain") or "").strip()


def get_daily_trend(product_id: str, domain: str) -> pd.DataFrame:
    if not product_id or not domain:
        return pd.DataFrame()

    states = sql_df(
        """
        SELECT observed_at, price, is_available
        FROM product_states
        WHERE product_id = ?
        ORDER BY observed_at
        """,
        params=(product_id,),
    )
    if states is None or states.empty:
        states = pd.DataFrame(columns=["observed_at", "price", "is_available"])
    states["observed_at"] = pd.to_datetime(states["observed_at"], errors="coerce")
    states = states.dropna(subset=["observed_at"])
    states["date"] = states["observed_at"].dt.date
    states["price"] = pd.to_numeric(states["price"], errors="coerce")

    if not states.empty:
        price_daily = (
            states.groupby("date")["price"]
            .agg(price_min="min", price_max="max")
            .reset_index()
        )
        stock_daily = (
            states.sort_values("observed_at")
            .groupby("date")["is_available"]
            .last()
            .reset_index()
            .rename(columns={"is_available": "stock_last"})
        )
        out = price_daily.merge(stock_daily, on="date", how="outer")
    else:
        out = pd.DataFrame(columns=["date", "price_min", "price_max", "stock_last"])

    ads = sql_df(
        """
        SELECT observed_at, batch_id
        FROM ad_impressions
        WHERE product_id = ? AND domain = ?
        ORDER BY observed_at
        """,
        params=(product_id, domain),
    )
    if ads is None or ads.empty:
        ads = pd.DataFrame(columns=["observed_at", "batch_id"])
    ads["observed_at"] = pd.to_datetime(ads["observed_at"], errors="coerce")
    ads = ads.dropna(subset=["observed_at"])
    if not ads.empty:
        ads["date"] = ads["observed_at"].dt.date
        by_batch = ads.groupby(["date", "batch_id"]).size().reset_index(name="ad_cnt")
        ad_daily = by_batch.groupby("date")["ad_cnt"].max().reset_index(name="ad_count")
        out = out.merge(ad_daily, on="date", how="outer")
    else:
        out["ad_count"] = 0

    out = out.sort_values("date")
    out["date"] = pd.to_datetime(out["date"], errors="coerce")
    out = out.dropna(subset=["date"])
    return out


def classify_landing_page(url: str) -> str:
    try:
        path = (urlparse(url).path or "").strip().lower()
        if path in {"", "/"}:
            return "é¦–é¡µ (Home)"
        if "/products/" in path:
            return "å•å“ç›´è¿ž (PDP)"
        if "/collections/" in path:
            return "é›†åˆé¡µ (Collection)"
        return "Other"
    except Exception:
        return "Other"


# åˆå§‹åŒ– DBï¼ˆä¸å½±å“ UI/çˆ¬è™«ä¸»æµç¨‹ï¼Œå¤±è´¥ä¹Ÿä¸é˜»æ–­ï¼‰
try:
    init_sqlite_schema()
except Exception as _e:
    print(f"SQLite init failed: {_e}")


def parse_shopify_product(html_content: str) -> dict:
    """è§£æž Shopify å•†å“é¡µï¼ˆå°½é‡åªç”¨ JSON-LD + OG availabilityï¼‰ã€‚

    è¿”å›žï¼š{price, currency, compare_at_price, is_available}
    è§£æžå¤±è´¥æ—¶è¿”å›žç©ºå­—æ®µï¼Œä¸æŠ›å¼‚å¸¸ã€‚
    """
    result = {"price": None, "currency": None, "compare_at_price": None, "is_available": None}
    html = html_content or ""

    def _normalize_availability(val: str) -> int | None:
        if not val:
            return None
        v = str(val).strip().lower()
        v = v.replace("https://schema.org/", "").replace("http://schema.org/", "")
        if v in {"instock", "in stock", "in_stock"}:
            return 1
        if v in {"outofstock", "out of stock", "out_of_stock"}:
            return 0
        return None

    try:
        # OG availability
        m_og = re.search(r'<meta[^>]+property=["\']og:availability["\'][^>]+content=["\']([^"\']+)["\']', html, re.I)
        if m_og:
            result["is_available"] = _normalize_availability(m_og.group(1))
    except Exception:
        pass

    # JSON-LD offers
    try:
        scripts = re.findall(r'<script[^>]+type=["\']application/ld\+json["\'][^>]*>(.*?)</script>', html, flags=re.I | re.S)
        for raw in scripts:
            raw = raw.strip()
            if not raw:
                continue
            try:
                data = json.loads(raw)
            except Exception:
                continue

            candidates = data if isinstance(data, list) else [data]
            for obj in candidates:
                if not isinstance(obj, dict):
                    continue
                offers = obj.get("offers")
                if isinstance(offers, list) and offers:
                    offers = offers[0]
                if isinstance(offers, dict):
                    if result["price"] is None:
                        p = offers.get("price")
                        if p is not None and str(p).strip() != "":
                            try:
                                result["price"] = float(p)
                            except Exception:
                                pass
                    if result["currency"] is None:
                        cur = offers.get("priceCurrency")
                        if cur:
                            result["currency"] = str(cur).strip()
                    if result["is_available"] is None:
                        avail = offers.get("availability") or obj.get("availability")
                        result["is_available"] = _normalize_availability(avail)
                # compare_at_priceï¼šJSON-LD é€šå¸¸æ²¡æœ‰ï¼Œç•™ç©º
            if result["price"] is not None or result["is_available"] is not None:
                break
    except Exception:
        pass

    return result


def db_upsert_product_and_state(
    *,
    url: str,
    domain: str,
    title: str,
    image_url: str | None,
    observed_at: str,
    price: float | None,
    compare_at_price: float | None,
    is_available: int | None,
    currency: str | None,
) -> str:
    product_id = _url_md5(url)
    handle = ""
    try:
        p = urlparse(url).path or ""
        m = re.search(r"/products/([^/?#]+)", p, re.I)
        if m:
            handle = m.group(1)[:200]
    except Exception:
        pass

    with DB_LOCK:
        conn = _db_connect()
        try:
            conn.execute(
                "INSERT OR IGNORE INTO products (id, domain, title, handle, image_url, first_seen_at) VALUES (?, ?, ?, ?, ?, ?)",
                (product_id, domain, title, handle, image_url, observed_at),
            )
            # è‹¥å·²å­˜åœ¨åˆ™å°½é‡è¡¥å…¨å¯è¯»å­—æ®µ
            conn.execute(
                """
                UPDATE products
                SET
                    domain = COALESCE(NULLIF(domain, ''), ?),
                    title = COALESCE(NULLIF(title, ''), ?),
                    handle = COALESCE(NULLIF(handle, ''), ?),
                    image_url = COALESCE(NULLIF(image_url, ''), ?)
                WHERE id = ?
                """,
                (domain, title, handle, image_url, product_id),
            )
            conn.execute(
                "INSERT INTO product_states (product_id, observed_at, price, compare_at_price, is_available, currency) VALUES (?, ?, ?, ?, ?, ?)",
                (product_id, observed_at, price, compare_at_price, is_available, currency),
            )
            conn.commit()
        finally:
            conn.close()
    return product_id


def db_insert_ad_impression(
    *,
    observed_at: str,
    keyword: str,
    gad_campaignid: str,
    url: str,
    raw_url: str,
    domain: str,
    brand: str,
    ad_type: str,
    ad_signals: str,
    product_id: str,
    batch_id: str,
    campaign: str,
) -> None:
    with DB_LOCK:
        conn = _db_connect()
        try:
            conn.execute(
                """
                INSERT INTO ad_impressions (
                    observed_at, keyword, gad_campaignid, url, raw_url, domain, brand, ad_type, ad_signals, product_id, batch_id, campaign
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """,
                (
                    observed_at,
                    keyword,
                    gad_campaignid,
                    url,
                    raw_url,
                    domain,
                    brand,
                    ad_type,
                    ad_signals,
                    product_id,
                    batch_id,
                    campaign,
                ),
            )
            conn.commit()
        finally:
            conn.close()


def load_history_file(filename: str) -> pd.DataFrame | None:
    path = os.path.join(HISTORY_DIR, filename)
    if os.path.exists(path):
        return pd.read_csv(path)
    return None


def save_to_history(df: pd.DataFrame, keyword_summary: str) -> str:
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    safe_summary = "".join([c for c in keyword_summary if c.isalnum() or c in (" ", "_")]).strip()
    filename = f"{timestamp}_{safe_summary}.csv"
    path = os.path.join(HISTORY_DIR, filename)
    df.to_csv(path, index=False)
    return filename


def rename_history_file(old_name: str, new_name: str) -> bool:
    if not new_name.endswith(".csv"):
        new_name += ".csv"
    old_path = os.path.join(HISTORY_DIR, old_name)
    new_path = os.path.join(HISTORY_DIR, new_name)
    if os.path.exists(old_path):
        os.rename(old_path, new_path)
        return True
    return False


def _infer_campaign_and_batch_from_filename(filename: str) -> tuple[str, str]:
    """ä»ŽåŽ†å²æ–‡ä»¶åæŽ¨æ–­ batch_id ä¸Ž campaign åï¼ˆç”¨äºŽå…¼å®¹æ—§åŽ†å²ï¼‰"""
    base = (filename or "").replace(".csv", "")
    parts = base.split("_")
    batch_id = ""
    campaign = ""
    if len(parts) >= 2 and parts[0].isdigit():
        batch_id = f"{parts[0]}_{parts[1]}"
        if len(parts) >= 3:
            # æ–°æ ¼å¼ï¼š{timestamp}_{campaign}_{summary}
            campaign = parts[2]
    return campaign, batch_id


def enrich_dataframe_for_ui(df: pd.DataFrame, history_filename: str = "") -> pd.DataFrame:
    """å¯¹æ—§åŽ†å²æ•°æ®åšå­—æ®µè¡¥å…¨ï¼Œç¡®ä¿æ–° UIï¼ˆDomain->Campaignï¼‰èƒ½æ¸²æŸ“ã€‚"""
    if df is None or df.empty:
        return df
    df = df.copy()

    inferred_campaign, inferred_batch_id = _infer_campaign_and_batch_from_filename(history_filename)
    if "Campaign" not in df.columns:
        df["Campaign"] = inferred_campaign
    else:
        df["Campaign"] = df["Campaign"].fillna("")
    if "Batch ID" not in df.columns:
        df["Batch ID"] = inferred_batch_id
    else:
        df["Batch ID"] = df["Batch ID"].fillna("")
    if "Campaign ID" not in df.columns:
        if "Final URL" in df.columns:
            df["Campaign ID"] = df["Final URL"].astype(str).apply(_extract_campaign_id_from_url)
        else:
            df["Campaign ID"] = ""
    else:
        df["Campaign ID"] = df["Campaign ID"].fillna("")

    if "Final URL" in df.columns and "sku_id" not in df.columns:
        df["sku_id"] = df["Final URL"].astype(str).apply(sku_fingerprint)

    if "Domain" in df.columns:
        def _brand_from_domain(domain: str) -> str:
            base = (domain or "").split(":")[0].lower()
            parts = base.split(".")
            core = parts[-2] if len(parts) >= 2 else base
            return (core[:1].upper() + core[1:]) if core else ""
        if "Brand" not in df.columns:
            df["Brand"] = df["Domain"].astype(str).apply(_brand_from_domain)

    if "Final URL" in df.columns and "Page Type" not in df.columns:
        def _page_type(url: str) -> str:
            u = (url or "").lower()
            if any(x in u for x in ["/product/", "/item/", "/p/"]):
                return "Product Page (è¯¦æƒ…é¡µ)"
            if any(x in u for x in ["/collection/", "/category/"]):
                return "Collection (åˆ—è¡¨é¡µ)"
            return "Other/Home"
        df["Page Type"] = df["Final URL"].astype(str).apply(_page_type)

    # å…¼å®¹æ—§è®°å½•ï¼šç¡®ä¿å…³é”®åˆ—å­˜åœ¨
    if "parse_strategy" not in df.columns:
        df["parse_strategy"] = ""
    return df


def adparser_enrich_and_dedup(df: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:
    """AdParserï¼šè§£æžå¹¿å‘Šå­—æ®µå¹¶ç”ŸæˆåŽ»é‡è§†å›¾ã€‚

    - ä¸»é”®ï¼šgad_campaignidï¼ˆå¿…é¡»é¡¹ï¼Œç¼ºå¤±åˆ™ä¸å‚ä¸ŽåŽ»é‡ï¼‰
    - æ¬¡è¦é”®ï¼šadgroupidã€Brandã€Product
    - å”¯ä¸€æ ‡è¯†ï¼šgad_campaignid + product_name + date
    """
    if df is None or df.empty:
        return df, df

    out = df.copy()

    if "Final URL" in out.columns:
        out["gad_campaignid"] = out["Final URL"].astype(str).apply(lambda u: _get_query_param(u, ["gad_campaignid"]))
        out["adgroupid"] = out["Final URL"].astype(str).apply(lambda u: _get_query_param(u, ["adgroupid", "adgroup_id"]))
    else:
        out["gad_campaignid"] = ""
        out["adgroupid"] = ""

    # è§„èŒƒåŒ– product_name
    if "Product" in out.columns:
        out["product_name"] = out["Product"].astype(str).fillna("").str.strip()
    else:
        out["product_name"] = ""

    # date
    if "Timestamp" in out.columns:
        out["date"] = pd.to_datetime(out["Timestamp"], errors="coerce").dt.strftime("%Y-%m-%d")
        out["date"] = out["date"].fillna("")
    else:
        out["date"] = ""

    # å”¯ä¸€é”®ï¼ˆgad_campaignid å¿…é¡»å­˜åœ¨ï¼Œå¦åˆ™ä¸å‚ä¸ŽåŽ»é‡ï¼‰
    out["ad_unique_key"] = out.apply(
        lambda r: f"{r.get('gad_campaignid','')}|{r.get('product_name','')}|{r.get('date','')}" if str(r.get("gad_campaignid", "")).strip() else "",
        axis=1,
    )

    # åªå¯¹å…·å¤‡ä¸»é”®çš„å¹¿å‘ŠåšåŽ»é‡ç»Ÿè®¡
    has_key = out["ad_unique_key"].astype(str).str.len() > 0
    if has_key.any():
        counts = out.loc[has_key].groupby("ad_unique_key").size().rename("å‡ºçŽ°æ¬¡æ•°")
        out = out.merge(counts, left_on="ad_unique_key", right_index=True, how="left")
    else:
        out["å‡ºçŽ°æ¬¡æ•°"] = 1
    out["å‡ºçŽ°æ¬¡æ•°"] = out["å‡ºçŽ°æ¬¡æ•°"].fillna(1).astype(int)

    # åŽ»é‡è§†å›¾ï¼šæ¯ä¸ª unique_key åªä¿ç•™ä¸€æ¡ï¼ˆæ— ä¸»é”®çš„ä¿ç•™åŽŸæ ·ï¼Œå‡ºçŽ°æ¬¡æ•°=1ï¼‰
    dedup_parts = []
    if has_key.any():
        dedup_parts.append(out.loc[has_key].sort_values(["ad_unique_key"]).drop_duplicates("ad_unique_key", keep="first"))
    if (~has_key).any():
        tmp = out.loc[~has_key].copy()
        tmp["å‡ºçŽ°æ¬¡æ•°"] = 1
        dedup_parts.append(tmp)
    dedup_df = pd.concat(dedup_parts, ignore_index=True) if dedup_parts else out.copy()

    return out, dedup_df


def ensure_domain_and_product_id(df: pd.DataFrame) -> pd.DataFrame:
    if df is None or df.empty:
        return df
    out = df.copy()
    if "domain" not in out.columns:
        if "Domain" in out.columns:
            out["domain"] = out["Domain"].astype(str)
        elif "Final URL" in out.columns:
            out["domain"] = out["Final URL"].astype(str).apply(lambda u: urlparse(u).netloc)
        else:
            out["domain"] = ""
    if "product_id" not in out.columns and "Final URL" in out.columns:
        out["product_id"] = out["Final URL"].astype(str).apply(_url_md5)
    return out


# --- é¡µé¢é…ç½® ---
st.set_page_config(page_title="ç«žå“åˆ†æž v15 é‡æž„ç‰ˆ", layout="wide", page_icon="ðŸ§ ")
st.title("ðŸš€ ç«žå“åˆ†æž (v15 æž¶æž„é‡æž„ç‰ˆ)")
st.markdown("å¤šè½®ä¼šè¯ Â· æ•°æ®å¯¹æ¯” Â· å¤šæ¨¡åž‹åˆ‡æ¢ Â· å¯æŠ˜å å·¥ä½œå°")

# --- Session State æ ¸å¿ƒæ•°æ®ç»“æž„ ---
if "current_df" not in st.session_state:
    st.session_state.current_df = None
if "current_history_key" not in st.session_state:
    st.session_state.current_history_key = None  # å½“å‰åŽ†å²æ–‡ä»¶åæˆ– "_current"

if "selected_domain" not in st.session_state:
    st.session_state.selected_domain = None
if "engine_should_analyze" not in st.session_state:
    st.session_state.engine_should_analyze = False
if "ai_report_content" not in st.session_state:
    st.session_state.ai_report_content = ""
# å½“å‰ä¼šè¯ï¼ˆå¤šè½®æ¶ˆæ¯ + å¿«ç…§ï¼‰ï¼Œä¸Ž current_history_key å¯¹åº”
if "current_session" not in st.session_state:
    st.session_state.current_session = {
        "id": None,
        "title": "",
        "messages": [],
        "created_at": None,
        "updated_at": None,
        "data_snapshots": [],  # å†…å­˜ä¸­çš„ [{timestamp, row_count, description, df}]
    }
# ç”¨äºŽ Diff çš„ä¸Šä¸€å¿«ç…§ï¼ˆä»…ä¿ç•™ä¸Šä¸€ä¸ª DataFrame å¼•ç”¨ä»¥ä¾¿å¯¹æ¯”ï¼‰
if "prev_snapshot_df" not in st.session_state:
    st.session_state.prev_snapshot_df = None
# LLM é…ç½®ï¼ˆç”±ä¾§è¾¹æ å†™å…¥ï¼Œä¸»åŒºè¯»å–ï¼‰
if "llm_platform" not in st.session_state:
    st.session_state.llm_platform = "DeepSeek"
if "llm_model" not in st.session_state:
    st.session_state.llm_model = "deepseek-chat"
if "llm_api_key" not in st.session_state:
    st.session_state.llm_api_key = ""
if "llm_base_url" not in st.session_state:
    st.session_state.llm_base_url = "https://api.deepseek.com"
# ä»»åŠ¡çŠ¶æ€æœºï¼šçˆ¬è™«è¿è¡Œä¸­ / æš‚åœ
if "is_running" not in st.session_state:
    st.session_state.is_running = True
# å¼‚æ­¥æŠ¥å‘Šç”Ÿæˆä¸­
if "report_generating" not in st.session_state:
    st.session_state.report_generating = False
# ä¸Šä¸€è½®æŠ“å–æ•°æ®ï¼ˆç”¨äºŽå¹¿å‘Šç»„å¯¹æ¯”ï¼‰
if "prev_run_df" not in st.session_state:
    st.session_state.prev_run_df = None

if "history_auto_loaded" not in st.session_state:
    st.session_state.history_auto_loaded = False

if (
    (not st.session_state.history_auto_loaded)
    and st.session_state.current_df is None
    and not st.session_state.get("current_history_key")
):
    try:
        files = list_history_files_with_sessions()
        if files:
            selected_file = files[0]
            loaded_df = load_history_file(selected_file)
            if loaded_df is not None:
                if "å¹¿å‘Šç»„ID" not in loaded_df.columns and "Domain" in loaded_df.columns:
                    loaded_df["å¹¿å‘Šç»„ID"] = loaded_df["Domain"]
                loaded_df = enrich_dataframe_for_ui(loaded_df, selected_file)
                st.session_state.current_df = loaded_df
                st.session_state.current_history_key = selected_file
                st.session_state._sidebar_selected_file = selected_file  # åŒæ­¥ä¾§è¾¹æ æ˜¾ç¤º
                st.session_state.ai_report_content = ""
                session = get_session_for_file(selected_file)
                st.session_state.current_session = {
                    "id": session.get("id"),
                    "title": session.get("title", selected_file),
                    "messages": session.get("messages", []),
                    "created_at": session.get("created_at"),
                    "updated_at": session.get("updated_at"),
                    "data_snapshots": [],
                }
    except Exception:
        pass
    st.session_state.history_auto_loaded = True


def append_row_to_csv(path: str, row_dict: dict, write_header: bool = False) -> None:
    """è¿½åŠ å•è¡Œåˆ° CSVï¼Œé¦–æ¬¡å¯å†™è¡¨å¤´ã€‚"""
    file_exists = os.path.exists(path)
    with open(path, "a", encoding="utf-8-sig", newline="") as f:
        w = csv.DictWriter(f, fieldnames=list(row_dict.keys()))
        if not file_exists or write_header:
            w.writeheader()
        w.writerow(row_dict)


# --- ä¾§è¾¹æ ï¼ˆå…¨éƒ¨å¯æŠ˜å ï¼‰---
with st.sidebar:
    st.header("ðŸŽ›ï¸ å…¨å±€æŽ§åˆ¶å°")

    # --- å…¨å±€åŸŸåè¿‡æ»¤ï¼ˆç½®é¡¶ï¼Œå¼ºåˆ¶å½±å“å…¨é¡µé¢ï¼‰---
    try:
        domains = []
        if st.session_state.current_df is not None and not st.session_state.current_df.empty:
            tmp_df = ensure_domain_and_product_id(st.session_state.current_df)
            domains = sorted([d for d in tmp_df["domain"].dropna().astype(str).unique().tolist() if d.strip()])
        if not domains:
            db_domains = load_sqlite_table_as_df(
                "SELECT DISTINCT domain FROM ad_impressions WHERE domain IS NOT NULL AND domain != ''"
            )
            if db_domains is not None and not db_domains.empty:
                domains = sorted([d for d in db_domains["domain"].astype(str).unique().tolist() if d.strip()])
        if domains:
            options = ["æ··åˆ"] + domains
            if len(domains) == 1 and not st.session_state.get("selected_domain"):
                selected_label = domains[0]
            else:
                selected_label = "æ··åˆ" if not st.session_state.get("selected_domain") else str(st.session_state.selected_domain)
            default_idx = options.index(selected_label) if selected_label in options else 0
            picked = st.selectbox("é€‰æ‹©ç«žäº‰å¯¹æ‰‹", options=options, index=default_idx)
            st.session_state.selected_domain = None if picked == "æ··åˆ" else picked
        else:
            st.session_state.selected_domain = None
    except Exception:
        pass

    # --- ä¾§è¾¹æ å¯¼èˆª ---
    page = "ðŸ“Š æ€»è§ˆ"

    with st.expander("ðŸ—„ï¸ åŽ†å²æ¡£æ¡ˆç®¡ç†", expanded=False):
        files = list_history_files_with_sessions()
        # è®¡ç®—é»˜è®¤é€‰ä¸­ç´¢å¼•ï¼šå¦‚æžœæ˜¯è‡ªåŠ¨åŠ è½½çš„æ–‡ä»¶ï¼Œæ˜¾ç¤ºå®ƒï¼›å¦åˆ™é»˜è®¤ç¬¬ä¸€é¡¹
        options = ["-- è¯·é€‰æ‹© --"] + files
        default_index = 0
        if st.session_state.get("_sidebar_selected_file") and st.session_state._sidebar_selected_file in files:
            default_index = options.index(st.session_state._sidebar_selected_file)
        selected_file = st.selectbox("é€‰æ‹©åŽ†å²è®°å½•", options, index=default_index, key="sb_history_select")
        col_h1, col_h2 = st.columns(2)
        if col_h1.button("ðŸ“‚ åŠ è½½æ•°æ®"):
            if selected_file and selected_file != "-- è¯·é€‰æ‹© --":
                loaded_df = load_history_file(selected_file)
                if loaded_df is not None:
                    if "å¹¿å‘Šç»„ID" not in loaded_df.columns and "Domain" in loaded_df.columns:
                        loaded_df["å¹¿å‘Šç»„ID"] = loaded_df["Domain"]
                    loaded_df = enrich_dataframe_for_ui(loaded_df, selected_file)
                    st.session_state.current_df = loaded_df
                    st.session_state.current_history_key = selected_file
                    st.session_state._sidebar_selected_file = selected_file  # åŒæ­¥ä¾§è¾¹æ æ˜¾ç¤º
                    st.session_state.ai_report_content = ""
                    # åŠ è½½è¯¥åŽ†å²è®°å½•å¯¹åº”çš„å¤šè½®ä¼šè¯
                    session = get_session_for_file(selected_file)
                    st.session_state.current_session = {
                        "id": session.get("id"),
                        "title": session.get("title", selected_file),
                        "messages": session.get("messages", []),
                        "created_at": session.get("created_at"),
                        "updated_at": session.get("updated_at"),
                        "data_snapshots": [],
                    }
                    st.success(f"å·²åŠ è½½: {selected_file}ï¼Œå¯ç»§ç»­å¤šè½®å¯¹è¯")
                    st.rerun()
        new_file_name = st.text_input("é‡å‘½åä¸º", placeholder="ä¾‹å¦‚: Tent_Analysis")
        if col_h2.button("âœï¸ é‡å‘½å"):
            if selected_file and new_file_name:
                if rename_history_file(selected_file, new_file_name):
                    st.success("é‡å‘½åæˆåŠŸï¼")
                    time.sleep(0.5)
                    st.rerun()

    with st.expander("ðŸ“¡ æ–°å»ºæŠ“å–ä»»åŠ¡", expanded=True):
        target_domain = st.text_input("ðŸŽ¯ ç›®æ ‡åŸŸå (ç•™ç©ºæŠ“æ‰€æœ‰)", value="", placeholder="ä¾‹å¦‚ naturehike.com")
        campaign_name = st.text_input("ðŸ“Œ Campaign Name (ä¸“é¡¹è®¡åˆ’)", value="", placeholder="ä¾‹å¦‚ï¼š2026æ˜¥å­£å¤§ä¿ƒ")
        keywords_input = st.text_area("ðŸ”‘ å…³é”®è¯åˆ—è¡¨ (é€—å·åˆ†éš”)", value="ultralight tent, camping chair", height=80)
        c_set1, c_set2 = st.columns(2)
        max_workers = c_set1.slider("å¹¶å‘çª—å£", 1, 4, 1)  # é™ä½Žé»˜è®¤å¹¶å‘ï¼Œå‡å°‘å¼‚å¸¸
        pages_to_scrape = c_set2.slider("å•è¯é¡µæ•°", 1, 3, 1)
        enable_global_lock = st.toggle("ðŸŒ å…¨çƒé”å®š (æ¨¡æ‹Ÿå½“åœ°æœç´¢)", value=True)
        target_region = st.selectbox(
            "ç›®æ ‡å¸‚åœº",
            ["ðŸ‡ºðŸ‡¸ ç¾Žå›½ (us/en)", "ðŸ‡¬ðŸ‡§ è‹±å›½ (uk/en)", "ðŸ‡©ðŸ‡ª å¾·å›½ (de/de)"],
            disabled=not enable_global_lock,
        )
        force_headless = st.checkbox("ðŸ™ˆ åŽå°é™é»˜è¿è¡Œ", value=False)
        use_proxy = st.checkbox("å¯ç”¨ä»£ç† IP", value=False)
        proxy_url = st.text_input("ä»£ç†åœ°å€", placeholder="http://127.0.0.1:7890", disabled=not use_proxy)
        auto_refresh = st.checkbox("è‡ªåŠ¨åˆ·æ–°è¿›åº¦(ä½Žé¢‘)", value=False)
        refresh_interval = st.select_slider("åˆ·æ–°é—´éš”(ç§’)", options=[2, 3, 5, 8, 10], value=5, disabled=not auto_refresh)
        start_btn = st.button("ðŸš€ å¯åŠ¨å¼ºåŠ›å¼•æ“Ž", type="primary", width="stretch")
        if st.button("â¹ï¸ åœæ­¢å¹¶åˆ†æž", key="stop_analyze_btn"):
            with ENGINE_LOCK:
                ENGINE_STATE["stop_requested"] = True
            st.session_state.engine_should_analyze = True
            st.session_state.is_running = False
            st.toast("å·²å‘é€åœæ­¢ä¿¡å·ï¼šå°†å…³é—­çª—å£å¹¶åˆ†æžå·²åŠ è½½æ•°æ®...")
            st.rerun()

        if st.button("ðŸ§¹ é‡ç½®å¼•æ“Ž(å¡æ­»ç”¨)", key="reset_engine_btn", width="stretch"):
            with ENGINE_LOCK:
                ENGINE_STATE["stop_requested"] = True
                ENGINE_STATE["running"] = False
                ENGINE_STATE["error"] = None
                ENGINE_STATE["pending"] = 0
                ENGINE_STATE["discovered"] = 0
                ENGINE_STATE["done"] = 0
                ENGINE_STATE["fail"] = 0
                ENGINE_STATE["shared_list"] = None
                ENGINE_STATE["transferred"] = False
            st.session_state.is_running = False
            st.session_state.engine_should_analyze = False
            st.toast("å·²é‡ç½®å¼•æ“ŽçŠ¶æ€ï¼Œå¯é‡æ–°å¯åŠ¨ã€‚è‹¥ä»æœ‰æ®‹ç•™çª—å£ï¼Œè¯·æ‰‹åŠ¨å…³é—­ã€‚")
            st.rerun()

        st.divider()

    with st.expander("ðŸ¤– AI æŒ‡æŒ¥ä¸­å¿ƒ (ç‚¹å‡»å±•å¼€/æ”¶èµ·)", expanded=True):
        platform = st.selectbox("Select Platform", list(PLATFORM_MODELS.keys()), key="sb_platform")
        models = PLATFORM_MODELS.get(platform, [])
        model_name = st.selectbox("Select Model", models, key="sb_model")
        api_key = st.text_input("API Key", type="password", value=os.environ.get("OPENAI_API_KEY", ""), key="sb_apikey")
        # Base URL ä½¿ç”¨å¸¸é‡ï¼Œä¸å±•ç¤ºåœ¨ UI
        base_url = DEFAULT_BASE_URLS.get(platform, "")
        st.session_state.llm_platform = platform
        st.session_state.llm_model = model_name
        st.session_state.llm_api_key = api_key

    with st.expander("ðŸ› ï¸ å¼•æ“ŽçŠ¶æ€è°ƒè¯• (ç‚¹å‡»å±•å¼€)", expanded=False):
        with ENGINE_LOCK:
            dbg = {
                k: ENGINE_STATE.get(k)
                for k in [
                    "running",
                    "stop_requested",
                    "discovered",
                    "pending",
                    "done",
                    "fail",
                    "error",
                    "save_filename",
                    "transferred",
                ]
            }
        st.json(dbg)
        if st.button("ðŸ”Ž æ£€æµ‹ 8501 ç«¯å£å ç”¨", key="dbg_check_8501"):
            pids = _get_listening_pids_on_port(8501)
            st.write({"port": 8501, "listening_pids": pids})
            if len(pids) >= 2:
                st.warning("æ£€æµ‹åˆ°å¤šä¸ªè¿›ç¨‹ç›‘å¬ 8501ï¼šè¿™ä¼šå¯¼è‡´é¡µé¢è¿žåˆ°æ—§è¿›ç¨‹ï¼ŒæŒ‰é’®/è¿›åº¦å¼‚å¸¸ã€‚è¯·ç»“æŸå¤šä½™ PID åŽé‡å¯ã€‚")
        st.session_state.llm_base_url = base_url
        if st.button("ðŸ§ª æµ‹è¯• API è¿žé€šæ€§"):
            if not api_key and platform != "Ollama":
                st.error("è¯·å…ˆè¾“å…¥ API Key")
            else:
                try:
                    provider = get_provider(platform, model_name, api_key, base_url)
                    if provider and provider.test_connection():
                        st.success("âœ… API è¿žæŽ¥ç•…é€šï¼")
                    else:
                        st.info("Ollama æœ¬åœ°æ— éœ€ Keyï¼Œè¯·ç¡®ä¿æœåŠ¡å·²å¯åŠ¨")
                except Exception as e:
                    st.error(f"âŒ è¿žæŽ¥å¤±è´¥: {e}")
        st.caption("ðŸ“ æ·±åº¦æŠ¥å‘Šç”Ÿæˆè§„åˆ™ (Prompt)")
        default_prompt = "è¯·ä½œä¸ºä¸€ä½èµ„æ·±è·¨å¢ƒç”µå•†è¿è¥ä¸“å®¶ï¼Œåˆ†æžä»¥ä¸‹ç«žå“æ•°æ®ã€‚\nè¯·æ€»ç»“ï¼š\n1. å¸‚åœºä¸Šçš„çƒ­é—¨äº§å“å–ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ\n2. ç«žå“ä¸»è¦é‡‡ç”¨äº†ä»€ä¹ˆå¹¿å‘Šç­–ç•¥ï¼ˆå›¾æ–‡/æœç´¢ï¼‰ï¼Ÿ\n3. å¤´éƒ¨ç«žå“çš„åˆ†å¸ƒæƒ…å†µå¦‚ä½•ï¼Ÿ\n4. å¦‚æžœæˆ‘è¦è·Ÿå–ï¼Œæœ‰ä»€ä¹ˆå·®å¼‚åŒ–å»ºè®®ï¼Ÿ"
        global_ai_rule = st.text_area("åœ¨æ­¤ä¿®æ”¹ç”Ÿæˆè§„åˆ™", value=default_prompt, height=120, key="sb_prompt")
        if st.button("âš¡ ç«‹å³ç”Ÿæˆ/é‡ç»˜æŠ¥å‘Š"):
            if st.session_state.current_df is None:
                st.error("è¯·å…ˆæŠ“å–æˆ–åŠ è½½æ•°æ®ï¼")
            elif not api_key and platform != "Ollama":
                st.error("è¯·å¡«å†™ API Keyï¼")
            else:
                def _generate_report():
                    try:
                        st.session_state.report_generating = True
                        df = st.session_state.current_df
                        provider = get_provider(st.session_state.llm_platform, st.session_state.llm_model, st.session_state.llm_api_key, st.session_state.llm_base_url)
                        if not provider:
                            return
                        summary = df["Product"].value_counts().head(10).to_string()
                        cols = [c for c in ["Keyword", "Product", "Domain", "Type"] if c in df.columns]
                        data_sample = df[cols].head(40).to_csv(index=False) if cols else ""
                        full_prompt = f"{global_ai_rule}\n\nã€æ•°æ®ç»Ÿè®¡æ‘˜è¦ã€‘:\n{summary}\n\nã€è¯¦ç»†æ•°æ®æ ·æœ¬ (å‰40æ¡)ã€‘:\n{data_sample}"
                        content = provider.chat([{"role": "user", "content": full_prompt}], stream=False)
                        st.session_state.ai_report_content = content
                        snap = make_snapshot(df, "ç”ŸæˆæŠ¥å‘Šæ—¶")
                        st.session_state.current_session.setdefault("data_snapshots", []).append(snap)
                    except Exception as e:
                        st.session_state.report_error = str(e)
                    finally:
                        st.session_state.report_generating = False

                if "report_error" in st.session_state:
                    del st.session_state.report_error
                th = threading.Thread(target=_generate_report, daemon=True)
                th.start()
                st.toast("ðŸš€ æ·±åº¦æŠ¥å‘Šç”Ÿæˆä¸­... å¯ç»§ç»­æ“ä½œå…¶ä»– Tabï¼Œå®ŒæˆåŽè¯·åˆ·æ–°æˆ–åˆ‡æ¢è‡³ [æ·±åº¦æŠ¥å‘Š] æŸ¥çœ‹ã€‚")
                st.rerun()

# --- æŠ“å–é€»è¾‘ï¼ˆä¿ç•™ V1ï¼‰---
def get_product_name(url):
    try:
        path = urlparse(url).path
        name = path.split("/")[-1].replace("-", " ").replace(".html", "").title()
        if not name:
            return "Home Page / Brand"
        return name[:50]
    except Exception:
        return "Unknown"


def resolve_url_fast(url, proxy_url=None):
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0 Safari/537.36"}
    proxies = {"http": proxy_url, "https": proxy_url} if proxy_url else None
    try:
        # å¯¹ Google Ads ç‚¹å‡»é“¾æŽ¥ï¼ˆ/aclkï¼‰ä¼˜å…ˆä»Žå‚æ•°ä¸­ç›´æŽ¥æå–è½åœ°é¡µ
        try:
            from urllib.parse import parse_qs, urlparse

            u = urlparse(url)
            if "/aclk" in (u.path or ""):
                qs = parse_qs(u.query)
                for k in ["adurl", "url", "q"]:
                    v = qs.get(k)
                    if v and v[0].startswith("http"):
                        return v[0]
        except Exception:
            pass

        resp = requests.get(url, headers=headers, proxies=proxies, allow_redirects=True, timeout=10)
        # ä¸å†ç”¨ "google.com" åšå¦å®šåˆ¤æ–­ï¼š/aclk ç»å¸¸ä¼šåœç•™åœ¨ google è·³è½¬å±‚
        return resp.url
    except Exception:
        return None


# --- ç”Ÿäº§è€…ï¼šåœ¨ä¸»çº¿ç¨‹ä¸­æ‰«æé¡µé¢å¹¶æŽ¨å…¥é˜Ÿåˆ—ï¼ˆDrissionPage å¿…é¡»ä¸»çº¿ç¨‹è°ƒç”¨ï¼‰---
def producer_fill_queue_main_thread(browser, k_list, settings, link_queue, total_discovered, ui_placeholders):
    """åœ¨ä¸»çº¿ç¨‹æ‰§è¡Œï¼Œä¾¿äºŽæ¯æ­¥åˆ·æ–°çœ‹æ¿ã€‚ui_placeholders = (m_total, m_done, m_pending, m_fail, p_bar)ã€‚"""
    base = DEFAULT_TARGET_URL.rstrip("/")
    m_total, m_done, m_pending, m_fail, p_bar = ui_placeholders
    for ki, keyword in enumerate(k_list):
        if not st.session_state.is_running:
            break
        tab = None
        try:
            tab = browser.new_tab()
            gl, hl = settings["gl"], settings["hl"]
            for i in range(settings["pages"]):
                search_url = f"{base}/search?q={keyword}&start={i*10}&gl={gl}&hl={hl}&pws=0"
                try:
                    tab.get(search_url, timeout=25)
                    time.sleep(2)
                    tab.scroll.to_bottom()
                    time.sleep(1)
                    tab.scroll.to_top()
                    time.sleep(0.5)
                except Exception as e:
                    print(f"Page load {keyword} p{i}: {e}")
                try:
                    links = tab.eles("xpath://a[@href]")
                    for link in links:
                        try:
                            href = link.attr("href")
                            if href and "/aclk" in href:
                                link_queue.put((keyword, href))
                                total_discovered[0] += 1
                        except Exception:
                            continue
                except Exception as e:
                    print(f"Extract links {keyword}: {e}")
                # æ¯é¡µåŽåˆ·æ–°çœ‹æ¿
                m_total.metric("ðŸ” å‘çŽ°é“¾æŽ¥æ€»æ•°", total_discovered[0])
                m_pending.metric("â³ å¾…å¤„ç†", link_queue.qsize())
                time.sleep(random.uniform(1, 2))
            if tab:
                try:
                    tab.close()
                except Exception:
                    pass
                tab = None
        except Exception as e:
            print(f"Producer keyword {keyword}: {e}")
        finally:
            if tab:
                try:
                    tab.close()
                except Exception:
                    pass
    # ç»“æŸä¿¡å·ï¼šæ¯ä¸ª worker ä¸€ä¸ª
    for _ in range(settings.get("num_workers", 2)):
        link_queue.put(None)


def consumer_worker(link_queue, shared_list, lock, counters, is_running, settings, save_path):
    """ä»Žé˜Ÿåˆ—å– (keyword, raw_url)ï¼Œè§£æžåŽç¬¦åˆæ¡ä»¶åˆ™è¿½åŠ åˆ° shared_list å¹¶å†™æ–‡ä»¶ã€‚"""
    session = requests.Session()
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0 Safari/537.36",
        "Accept-Language": "en-US,en;q=0.9",
    }

    def _brand_fallback_from_domain(domain: str) -> str:
        base = domain.split(":")[0].lower()
        parts = base.split(".")
        if len(parts) >= 2:
            core = parts[-2]
        else:
            core = base
        return core[:1].upper() + core[1:]

    def _extract_brand(html: str, domain: str) -> str:
        try:
            m = re.search(r'<meta[^>]+property=["\']og:site_name["\'][^>]+content=["\']([^"\']+)["\']', html, re.I)
            if m and m.group(1).strip():
                return m.group(1).strip()[:80]
            m = re.search(r'<title[^>]*>(.*?)</title>', html, re.I | re.S)
            if m:
                title = re.sub(r"\s+", " ", m.group(1)).strip()
                if title:
                    # å¸¸è§æ ¼å¼ï¼šBrand - Product / Product | Brand
                    for sep in ["|", "-", "â€“", "â€”"]:
                        if sep in title:
                            cand = title.split(sep)[-1].strip()
                            if 2 <= len(cand) <= 40:
                                return cand
        except Exception:
            pass
        return _brand_fallback_from_domain(domain)

    def _page_type(url: str) -> str:
        u = (url or "").lower()
        if any(x in u for x in ["/product/", "/item/", "/p/"]):
            return "Product Page (è¯¦æƒ…é¡µ)"
        if any(x in u for x in ["/collection/", "/category/"]):
            return "Collection (åˆ—è¡¨é¡µ)"
        return "Other/Home"

    def _detect_mainstream_ad_signals(url: str, html: str = "") -> tuple[bool, list[str]]:
        """æ£€æµ‹æ˜¯å¦ä¸ºä¸»æµå¹³å°å¹¿å‘Šï¼Œè¿”å›ž (is_ad, signals)"""
        signals = []
        try:
            from urllib.parse import parse_qs, urlparse
            qs = parse_qs(urlparse(url).query)
            url_lower = url.lower()
            # Google Ads ç‚¹å‡»ç­¾åï¼š/aclk
            if "/aclk" in url_lower:
                signals.append("/aclk")
            # Google å¹¿å‘Šä¿¡å·
            if "gclid" in qs:
                signals.append("gclid")
            if any(k.lower() == "gad_source" and v == ["1"] for k, v in qs.items()):
                signals.append("gad_source=1")
            if any(k.lower() == "gad_campaignid" for k, v in qs.items()):
                signals.append("gad_campaignid")
            if any(k.lower() == "gbraid" for k, v in qs.items()):
                signals.append("gbraid")
            if html and 'data-ved' in html:
                signals.append("data-ved")
            # YouTube å¹¿å‘Šä¿¡å·
            if "si" in qs:
                signals.append("si")
            if any(k.lower() in {"feature", "lc", "pp"} for k, v in qs.items()):
                signals.append("youtube_params")
            if "ab_channel" in qs:
                signals.append("ab_channel")
            # Meta/Facebook/Instagram å¹¿å‘Šä¿¡å·
            if "fbclid" in qs:
                signals.append("fbclid")
            if "igshid" in qs:
                signals.append("igshid")
            if any(v and v[0].lower() in {"facebook", "instagram", "fb"} for k, v in qs.items() if k.lower() == "utm_source"):
                signals.append("utm_source=meta")
            # TikTok å¹¿å‘Šä¿¡å·
            if "ttclid" in qs:
                signals.append("ttclid")
            if any(v and v[0].lower() == "tiktok" for k, v in qs.items() if k.lower() == "utm_source"):
                signals.append("utm_source=tiktok")
            # X/Twitter å¹¿å‘Šä¿¡å·
            if "twclid" in qs:
                signals.append("twclid")
            if any(v and v[0].lower() in {"twitter", "x"} for k, v in qs.items() if k.lower() == "utm_source"):
                signals.append("utm_source=twitter")
            # LinkedIn å¹¿å‘Šä¿¡å·
            if "li_fat_id" in qs:
                signals.append("li_fat_id")
            if any(v and v[0].lower() == "linkedin" for k, v in qs.items() if k.lower() == "utm_source"):
                signals.append("utm_source=linkedin")
            # Bing å¹¿å‘Šä¿¡å·
            if "msclkid" in qs:
                signals.append("msclkid")
            # é€šç”¨å¹¿å‘Šä¿¡å·
            if any(v and v[0].lower() in {"cpc", "ppc"} for k, v in qs.items() if k.lower() == "utm_medium"):
                signals.append("utm_medium=cpc/ppc")
            # é€šç”¨ç»„åˆï¼šutm_campaign + utm_sourceï¼ˆå¸¸è§å¹¿å‘Šæ ‡è®°ï¼‰
            if "utm_campaign" in qs and any(k.lower() == "utm_source" for k, v in qs.items()):
                signals.append("utm_campaign+source")
        except Exception:
            pass
        return (len(signals) > 0, signals)

    while is_running[0]:
        try:
            item = link_queue.get(timeout=1)
            if item is None:
                break
            keyword, raw_url = item
            final_url = resolve_url_fast(raw_url, settings["proxy"])
            if not final_url:
                with lock:
                    counters["fail"] += 1
                continue
            domain_filter = settings.get("domain", "")
            if domain_filter and domain_filter not in final_url:
                continue
            ad_type = "Shopping (å›¾)" if "shopping" in raw_url else "Search (æ–‡)"
            domain = urlparse(final_url).netloc

            # --- ç¨³å®šæ€§åŠ å›ºï¼šrequests å•ä¾‹æŠ“å– + è§£æžä¸å…¨æ ‡è®°å¾…åˆ†æžï¼ˆæ¨¡å—äºŒï¼‰---
            parse_strategy = "A"
            title = get_product_name(final_url)
            price = None
            review_count = None
            http_status = None
            error_msg = None
            html_snapshot_id = None
            brand = _brand_fallback_from_domain(domain)
            page_type = _page_type(final_url)

            proxies = {"http": settings.get("proxy"), "https": settings.get("proxy")} if settings.get("proxy") else None
            html = ""
            shopify_price = None
            shopify_currency = None
            shopify_compare_at = None
            shopify_is_available = None
            image_url = None
            try:
                resp = session.get(final_url, headers=headers, proxies=proxies, timeout=10)
                http_status = resp.status_code
                html = resp.text or ""
                brand = _extract_brand(html, domain)
                try:
                    m_img = re.search(r'<meta[^>]+property=["\']og:image["\'][^>]+content=["\']([^"\']+)["\']', html, re.I)
                    if m_img and m_img.group(1).strip():
                        image_url = m_img.group(1).strip()[:500]
                except Exception:
                    pass
                # æ ‡é¢˜å…œåº•
                m_title = re.search(r'<meta[^>]+property=["\']og:title["\'][^>]+content=["\']([^"\']+)["\']', html, re.I)
                if m_title and m_title.group(1).strip():
                    title = m_title.group(1).strip()[:120]
                else:
                    m_title2 = re.search(r'<title[^>]*>(.*?)</title>', html, re.I | re.S)
                    if m_title2:
                        title = re.sub(r"\s+", " ", m_title2.group(1)).strip()[:120] or title
                # ä»·æ ¼/è¯„è®ºå¯å‘å¼
                price = infer_price_from_text(html[:4000])
                m_rev = re.search(r'(\d{1,6})\s*reviews?', html[:8000], re.I)
                if m_rev:
                    review_count = int(m_rev.group(1))

                # Shopify å•†å“è¯¦æƒ…è§£æžï¼ˆJSON-LD + OG availabilityï¼‰
                try:
                    shopify = parse_shopify_product(html)
                    shopify_price = shopify.get("price")
                    shopify_currency = shopify.get("currency")
                    shopify_compare_at = shopify.get("compare_at_price")
                    shopify_is_available = shopify.get("is_available")
                    if shopify_price is not None:
                        price = shopify_price
                except Exception as e:
                    # è§£æžå¤±è´¥ä¸å½±å“ä¸»æµç¨‹
                    error_msg = (error_msg + " | " if error_msg else "") + f"shopify_parse:{e}"
            except Exception as e:
                parse_strategy = "B_PENDING"
                error_msg = str(e)

            # --- å¹¿å‘Šæ ‡è®°ï¼šèƒ½è®¿é—®ä½†ä¸æ˜¯å¹¿å‘Š => æ ‡è®° Is Ad=Falseï¼Œä¸ç®—å¤±è´¥ ---
            # æ³¨æ„ï¼šraw_url å¯èƒ½åŒ…å« /aclkï¼Œå› æ­¤æŠŠ raw_url ä¸Ž final_url éƒ½çº³å…¥ä¿¡å·åˆ¤æ–­
            is_ad_1, ad_signals_1 = _detect_mainstream_ad_signals(raw_url, "")
            is_ad_2, ad_signals_2 = _detect_mainstream_ad_signals(final_url, html)
            ad_signals = list(dict.fromkeys(ad_signals_1 + ad_signals_2))
            is_ad = bool(ad_signals)

            # Search(æ–‡) æˆ–è§£æžç¼ºå¤±ï¼šä¿ç•™ HTML å¿«ç…§å¹¶æ ‡è®°å¾…åˆ†æžï¼Œä¸ç®—å¤±è´¥
            need_snapshot = (ad_type == "Search (æ–‡)") or (not title) or (price is None)
            if need_snapshot:
                if parse_strategy == "A":
                    parse_strategy = "B_PENDING"
                try:
                    html_snapshot_id = f"{sku_fingerprint(final_url)}_{int(datetime.now().timestamp())}"
                    snapshot_path = os.path.join(HISTORY_DIR, "snapshots", f"{html_snapshot_id}.html")
                    os.makedirs(os.path.dirname(snapshot_path), exist_ok=True)
                    with open(snapshot_path, "w", encoding="utf-8") as f:
                        f.write(html if html else ("<!-- fetch failed -->\n" + final_url))
                except Exception:
                    html_snapshot_id = None

            row = {
                "Campaign": settings.get("campaign", ""),
                "Campaign ID": _extract_campaign_id_from_url(final_url),
                "Batch ID": settings.get("batch_id", ""),
                "Keyword": keyword,
                "Product": title,
                "Brand": brand,
                "Domain": domain,
                "å¹¿å‘Šç»„ID": _extract_adgroup_id_from_url(final_url) or _extract_campaign_id_from_url(final_url) or domain,
                "Page Type": page_type,
                "Type": ad_type,
                "Is Ad": is_ad,
                "Ad Signals": ", ".join(ad_signals),
                "Final URL": final_url,
                "sku_id": sku_fingerprint(final_url),
                "Price": price,
                "Review Count": review_count,
                "Currency": shopify_currency,
                "Is Available": shopify_is_available,
                "Compare At Price": shopify_compare_at,
                "Timestamp": datetime.now().strftime("%Y-%m-%d %H:%M"),
                "parse_strategy": parse_strategy,
                "http_status": http_status,
                "html_snapshot_id": html_snapshot_id,
                "error_msg": error_msg,
            }

            # --- å…¥åº“ï¼ˆä¸é˜»æ–­ä¸»æµç¨‹ï¼‰---
            try:
                observed_at = row.get("Timestamp")
                product_id = db_upsert_product_and_state(
                    url=final_url,
                    domain=domain,
                    title=title,
                    image_url=image_url,
                    observed_at=observed_at,
                    price=price if price is not None else None,
                    compare_at_price=shopify_compare_at,
                    is_available=shopify_is_available,
                    currency=shopify_currency,
                )
                db_insert_ad_impression(
                    observed_at=observed_at,
                    keyword=keyword,
                    gad_campaignid=_extract_campaign_id_from_url(final_url),
                    url=final_url,
                    raw_url=raw_url,
                    domain=domain,
                    brand=brand,
                    ad_type=ad_type,
                    ad_signals=", ".join(ad_signals),
                    product_id=product_id,
                    batch_id=settings.get("batch_id", ""),
                    campaign=settings.get("campaign", ""),
                )
            except Exception as e:
                row["error_msg"] = (row.get("error_msg") + " | " if row.get("error_msg") else "") + f"db:{e}"
            with lock:
                shared_list.append(row)
                counters["done"] += 1
                append_row_to_csv(save_path, row, write_header=(len(shared_list) == 1))
        except queue.Empty:
            continue
        except Exception as e:
            with lock:
                counters["fail"] += 1
            print(f"Consumer error: {e}")


# --- æ‰§è¡ŒæŠ“å–ï¼ˆåŽå°çº¿ç¨‹ + å¯ä¸­æ–­ï¼‰---
if start_btn:
    k_list = [k.strip() for k in keywords_input.split(",") if k.strip()]
    if not k_list:
        st.error("è¯·è¾“å…¥è‡³å°‘ä¸€ä¸ªå…³é”®è¯")
    else:
        with ENGINE_LOCK:
            already_running = bool(ENGINE_STATE.get("running"))
        if already_running:
            st.warning("å¼•æ“Žæ­£åœ¨è¿è¡Œä¸­ï¼Œè¯·å…ˆåœæ­¢æˆ–ç­‰å¾…å®Œæˆã€‚")
        else:
            st.session_state.is_running = True
            st.session_state.engine_should_analyze = False
            # ä¿å­˜â€œä¸Šä¸€è½®â€ç”¨äºŽå¹¿å‘Šç»„å¯¹æ¯”
            st.session_state.prev_run_df = st.session_state.current_df.copy() if st.session_state.current_df is not None else None

            proxy_setting = proxy_url if use_proxy else None
            gl_code, hl_code = "us", "en"
            if enable_global_lock:
                if "è‹±å›½" in target_region:
                    gl_code, hl_code = "uk", "en"
                elif "å¾·å›½" in target_region:
                    gl_code, hl_code = "de", "de"

            task_start = datetime.now().strftime("%Y%m%d_%H%M%S")
            current_save_filename = None
            current_save_path = None
            existing_key = st.session_state.get("current_history_key")
            if isinstance(existing_key, str) and existing_key.endswith(".csv"):
                p = os.path.join(HISTORY_DIR, existing_key)
                if os.path.exists(p):
                    current_save_filename = existing_key
                    current_save_path = p
                    try:
                        with open(current_save_path, "w", encoding="utf-8-sig") as _f:
                            _f.write("")
                    except Exception:
                        pass
            if not current_save_filename or not current_save_path:
                safe_summary = "".join([c for c in k_list[0][:20] if c.isalnum() or c in (" ", "_")]).strip()
                safe_campaign = "".join([c for c in campaign_name[:20] if c.isalnum() or c in (" ", "_")]).strip()
                campaign_part = f"_{safe_campaign}" if safe_campaign else ""
                current_save_filename = f"{task_start}{campaign_part}_{safe_summary}.csv"
                current_save_path = os.path.join(HISTORY_DIR, current_save_filename)

            settings = {
                "proxy": proxy_setting,
                "domain": target_domain.strip(),
                "campaign": campaign_name.strip(),
                "pages": pages_to_scrape,
                "gl": gl_code,
                "hl": hl_code,
                "num_workers": max_workers,
                "batch_id": task_start,
                "force_headless": force_headless,
            }

            th = threading.Thread(
                target=_engine_thread_run,
                args=(k_list, settings, current_save_path, current_save_filename),
                daemon=True,
            )
            th.start()
            st.toast("å¼•æ“Žå·²åœ¨åŽå°å¯åŠ¨ï¼Œå¯éšæ—¶ç‚¹å‡»ã€Œåœæ­¢å¹¶åˆ†æžã€ã€‚")
            st.rerun()

# å¼•æ“Žè¿è¡Œä¸­ï¼šæ˜¾ç¤ºå®žæ—¶çœ‹æ¿å¹¶è‡ªåŠ¨åˆ·æ–°
with ENGINE_LOCK:
    eng_running = bool(ENGINE_STATE.get("running"))
    eng_err = ENGINE_STATE.get("error")
    eng_done = int(ENGINE_STATE.get("done") or 0)
    eng_fail = int(ENGINE_STATE.get("fail") or 0)
    eng_pending = int(ENGINE_STATE.get("pending") or 0)
    eng_disc = int(ENGINE_STATE.get("discovered") or 0)
    eng_save = ENGINE_STATE.get("save_filename")
    eng_shared = ENGINE_STATE.get("shared_list")
    eng_transferred = bool(ENGINE_STATE.get("transferred"))

if eng_running:
    st.divider()
    st.subheader("ðŸ“Š å®žæ—¶è¿›åº¦ç›‘æŽ§çœ‹æ¿ï¼ˆåŽå°è¿è¡Œï¼‰")
    col1, col2, col3, col4 = st.columns(4)
    col1.metric("ðŸ” å‘çŽ°é“¾æŽ¥æ€»æ•°", eng_disc)
    col2.metric("âœ… å·²å®Œæˆ", eng_done)
    col3.metric("â³ å¾…å¤„ç†", eng_pending)
    col4.metric("âŒ å¤±è´¥/å¼‚å¸¸", eng_fail)
    if st.button("ðŸ”„ åˆ·æ–°è¿›åº¦", key="engine_refresh_btn"):
        st.rerun()
    if auto_refresh:
        time.sleep(float(refresh_interval))
        st.rerun()

# å¼•æ“Žç»“æŸï¼šå¦‚æžœç”¨æˆ·ç‚¹äº†åœæ­¢å¹¶åˆ†æžï¼ˆæˆ–è‡ªç„¶ç»“æŸï¼‰ï¼ŒæŠŠæ•°æ®åˆ‡æ¢åˆ°åˆ†æžåŒºï¼ˆä»…ä¸€æ¬¡ï¼‰
if (
    (not eng_running)
    and (not eng_transferred)
    and eng_shared is not None
    and len(eng_shared) > 0
    and (st.session_state.get("engine_should_analyze") or st.session_state.get("is_running"))
):
    st.session_state.current_df = pd.DataFrame(list(eng_shared))
    st.session_state.current_history_key = eng_save
    st.session_state.current_session = {"id": None, "title": "", "messages": [], "created_at": None, "updated_at": None, "data_snapshots": []}
    st.session_state.is_running = False
    st.session_state.engine_should_analyze = False
    with ENGINE_LOCK:
        ENGINE_STATE["transferred"] = True
    st.toast(f"æ•°æ®å·²è‡ªåŠ¨æŒä¹…åŒ–: {eng_save}")
    st.rerun()

if (not eng_running) and eng_err:
    st.error(f"å¼•æ“Žå¯åŠ¨å¤±è´¥: {eng_err}")


# --- ä¸»å†…å®¹ï¼šæ•°æ®åˆ†æžå·¥ä½œå°ï¼ˆæ€»è§ˆï¼‰---
if page == "ðŸ“Š æ€»è§ˆ" and st.session_state.current_df is not None:
    df = enrich_dataframe_for_ui(st.session_state.current_df.copy(), st.session_state.current_history_key or "")
    df = ensure_domain_and_product_id(df)
    if st.session_state.get("selected_domain"):
        df = df[df["domain"] == st.session_state.selected_domain]
    if "å¹¿å‘Šç»„ID" not in df.columns and "Domain" in df.columns:
        df["å¹¿å‘Šç»„ID"] = df["Domain"]

    st.divider()
    st.header(f"ðŸ“ˆ æ•°æ®åˆ†æžå·¥ä½œå° (å…± {len(df)} æ¡)")
    csv_bytes = df.to_csv(index=False).encode("utf-8")
    st.download_button("ðŸ“¥ ä¸‹è½½å½“å‰æ•°æ® CSV", csv_bytes, "analysis_data.csv", "text/csv", key="dl_csv_main")

    tab_report, tab_ai, tab_adgroup, tab_decoder, tab_diff = st.tabs([
        "ðŸ“„ æ·±åº¦æŠ¥å‘Š",
        "ðŸ’¬ AI æ•°æ®é¡¾é—®",
        "ðŸ“ å¹¿å‘Šç»„è§†å›¾",
        "ðŸ§© æŠ•æ”¾ç­–ç•¥è§£å¯† (Ad Strategy Decoder)",
        "âš–ï¸ Batch Diff",
    ])

    with tab_report:
        with st.expander("ðŸ“‘ æ·±åº¦æŠ¥å‘Š (ç‚¹å‡»å±•å¼€)", expanded=True):
            if getattr(st.session_state, "report_generating", False):
                st.info("ðŸš€ æ·±åº¦æŠ¥å‘Šç”Ÿæˆä¸­... è¯·ç¨åŽåˆ·æ–°æˆ–å†æ¬¡åˆ‡æ¢åˆ°æ­¤ Tab æŸ¥çœ‹ã€‚")
            elif getattr(st.session_state, "report_error", None):
                st.error(f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {st.session_state.report_error}")
            elif st.session_state.ai_report_content:
                st.info("ðŸ’¡ å¦‚éœ€ä¿®æ”¹æŠ¥å‘Šæ–¹å‘ï¼Œè¯·åœ¨ä¾§è¾¹æ ã€ŒAI æŒ‡æŒ¥ä¸­å¿ƒã€ä¿®æ”¹ç”Ÿæˆè§„åˆ™å¹¶é‡æ–°ç”Ÿæˆã€‚")
                st.markdown(st.session_state.ai_report_content)
            else:
                st.warning("ðŸ‘ˆ æš‚æ— æŠ¥å‘Šã€‚è¯·åœ¨å·¦ä¾§ã€ŒAI æŒ‡æŒ¥ä¸­å¿ƒã€é…ç½® Platform/Model ä¸Ž API Key åŽç‚¹å‡»ã€Œç«‹å³ç”Ÿæˆ/é‡ç»˜æŠ¥å‘Šã€ã€‚")

    with tab_ai:
        chat_key = st.session_state.current_history_key or "_current"
        current_session = st.session_state.current_session
        messages = current_session.get("messages", [])

        with st.expander("ðŸ’¬ å¤šè½®æ•°æ®æŸ¥è¯¢ä¸Žå¯¹æ¯” (ç‚¹å‡»å±•å¼€)", expanded=True):
            st.caption("åœ¨å½“å‰åŽ†å²è®°å½•å†…å¤šè½®æé—®ï¼Œå¯è¦æ±‚å¯¹æ¯”å‰å‡ è½®ç»“è®ºã€‚")
            for msg in messages:
                with st.chat_message(msg.get("role", "user")):
                    st.markdown(msg.get("content", ""))

            if prompt := st.chat_input("ä¾‹å¦‚ï¼šæ ¹æ®æŠ¥å‘Šä¸»æŽ¨ä»€ä¹ˆæ¬¾å¼ï¼Ÿæˆ–ï¼šå¯¹æ¯”å‰å‡ è½®åˆ†æžç»“æžœ"):
                api_key = st.session_state.get("llm_api_key", "")
                platform = st.session_state.get("sb_platform", "OpenAI")
                if not api_key and platform != "Ollama":
                    st.error("è¯·åœ¨ä¾§è¾¹æ ã€ŒAI æŒ‡æŒ¥ä¸­å¿ƒã€é…ç½® API Key")
                else:
                    messages.append({"role": "user", "content": prompt})
                    with st.chat_message("user"):
                        st.markdown(prompt)

                    data_cols = [c for c in ["Product", "Domain", "Type", "å¹¿å‘Šç»„ID"] if c in df.columns]
                    data_context = df[data_cols].head(50).to_csv(index=False) if data_cols else ""
                    report_context = st.session_state.ai_report_content or "æš‚æœªç”Ÿæˆæ­£å¼æŠ¥å‘Š"
                    system_msg = f"""ä½ æ˜¯ç²¾é€šç”µå•†æ•°æ®çš„åˆ†æžåŠ©æ‰‹ã€‚ç”¨æˆ·ä¼šåœ¨åŒä¸€åŽ†å²è®°å½•å†…å¤šè½®æé—®ï¼Œå¯èƒ½è¦æ±‚å¯¹æ¯”ä¹‹å‰å‡ è½®çš„åˆ†æžç»“è®ºã€‚

ã€å½“å‰æ•°æ®ç‰‡æ®µã€‘:
{data_context}

ã€å·²ç”Ÿæˆçš„æˆ˜æœ¯åˆ†æžæŠ¥å‘Šã€‘:
{report_context}

è¯·ç»“åˆä»¥ä¸Šä¿¡æ¯å›žç­”ï¼›è‹¥ç”¨æˆ·è¦æ±‚ã€Œå¯¹æ¯”ã€æˆ–ã€Œå’Œä¹‹å‰ç›¸æ¯”ã€ï¼Œè¯·ç»¼åˆæœ¬å¯¹è¯ä¸­ä¹‹å‰çš„é—®ç­”è¿›è¡Œå¯¹æ¯”åˆ†æžã€‚"""

                    try:
                        provider = get_provider(st.session_state.llm_platform, st.session_state.llm_model, st.session_state.llm_api_key, st.session_state.llm_base_url)
                        with st.chat_message("assistant"):
                            message_placeholder = st.empty()
                            full_response = ""
                            msgs = [{"role": "system", "content": system_msg}] + [{"role": m["role"], "content": m["content"]} for m in messages[-10:]]
                            stream_result = provider.chat(msgs, stream=True)
                            for chunk in stream_result:
                                full_response += chunk
                                message_placeholder.markdown(full_response + "â–Œ")
                            message_placeholder.markdown(full_response)
                        messages.append({"role": "assistant", "content": full_response})
                        current_session["messages"] = messages
                        if chat_key != "_current":
                            save_session_for_file(chat_key, {
                                "id": current_session.get("id"),
                                "title": current_session.get("title"),
                                "messages": current_session["messages"],
                                "created_at": current_session.get("created_at"),
                                "updated_at": current_session.get("updated_at"),
                                "data_snapshots_meta": [{"timestamp": s.get("timestamp"), "row_count": s.get("row_count"), "description": s.get("description", "")} for s in current_session.get("data_snapshots", [])],
                            })
                    except Exception as e:
                        st.error(f"å¯¹è¯æœåŠ¡å‡ºé”™: {e}")

    with tab_adgroup:
        df_parsed, df_dedup = adparser_enrich_and_dedup(df)

        t_raw, t_tree = st.tabs(["è§†å›¾ 1ï¼šåŽŸå§‹æ•°æ®è¡¨", "è§†å›¾ 2ï¼šå¹¿å‘Šæ´»åŠ¨æž¶æž„ï¼ˆæ ‘çŠ¶å›¾ï¼‰"])

        with t_raw:
            show_cols = [
                c
                for c in [
                    "Timestamp",
                    "Brand",
                    "gad_campaignid",
                    "Product",
                    "å‡ºçŽ°æ¬¡æ•°",
                    "Type",
                    "Final URL",
                    "Keyword",
                    "parse_strategy",
                    "http_status",
                    "error_msg",
                ]
                if c in df_parsed.columns
            ]
            raw_df_show = df_parsed[show_cols].copy() if show_cols else df_parsed
            col_cfg = {}
            if "Final URL" in raw_df_show.columns:
                col_cfg["Final URL"] = st.column_config.LinkColumn("URL", display_text="æ‰“å¼€")
            st.dataframe(raw_df_show, width="stretch", column_config=col_cfg)

        with t_tree:
            with st.expander("ðŸ“ Brand â†’ Campaign ID(gad_campaignid) â†’ Productsï¼ˆåŽ»é‡åŽï¼‰", expanded=True):
                if df_dedup is None or df_dedup.empty:
                    st.info("æš‚æ— æ•°æ®")
                else:
                    # åªå±•ç¤ºå…·å¤‡ gad_campaignid çš„å¹¿å‘Šæ ‘
                    tree_df = df_dedup.copy()
                    if "gad_campaignid" not in tree_df.columns:
                        tree_df["gad_campaignid"] = ""
                    tree_df["gad_campaignid"] = tree_df["gad_campaignid"].fillna("")
                    tree_df = tree_df[tree_df["gad_campaignid"].astype(str).str.strip().ne("")]
                    if tree_df.empty:
                        st.info("æ²¡æœ‰è§£æžåˆ° gad_campaignidï¼ˆæ— æ³•æž„å»ºæ ‘çŠ¶å›¾ï¼‰ã€‚")
                    else:
                        if "Brand" not in tree_df.columns:
                            tree_df["Brand"] = ""
                        for brand, brand_df in tree_df.groupby("Brand", sort=False):
                            brand_name = brand if str(brand).strip() else "(Unknown Brand)"
                            with st.expander(f"ðŸ·ï¸ å“ç‰Œ: {brand_name}ï¼ˆ{len(brand_df)} æ¡å¹¿å‘Šï¼‰", expanded=False):
                                for cid, cid_df in brand_df.groupby("gad_campaignid", sort=False):
                                    cid_name = cid if str(cid).strip() else "(No gad_campaignid)"
                                    with st.expander(f"ðŸ“Œ Campaign ID: {cid_name}ï¼ˆ{len(cid_df)} ä¸ªäº§å“ï¼‰", expanded=False):
                                        show_cols2 = [
                                            c
                                            for c in [
                                                "product_name",
                                                "å‡ºçŽ°æ¬¡æ•°",
                                                "adgroupid",
                                                "Price",
                                                "Review Count",
                                                "Timestamp",
                                                "Final URL",
                                            ]
                                            if c in cid_df.columns
                                        ]
                                        view_df = cid_df[show_cols2].copy() if show_cols2 else cid_df
                                        col_cfg2 = {}
                                        if "Final URL" in view_df.columns:
                                            col_cfg2["Final URL"] = st.column_config.LinkColumn("URL", display_text="æ‰“å¼€")
                                        st.dataframe(view_df, width="stretch", column_config=col_cfg2)

    with tab_decoder:
        st.subheader("ðŸ§© æŠ•æ”¾ç­–ç•¥è§£å¯† (Ad Strategy Decoder)")
        try:
            selected_domain = st.session_state.get("selected_domain")
            ad_df = load_sqlite_table_as_df(
                """
                SELECT observed_at, keyword, gad_campaignid, url, raw_url, domain, brand, ad_type, ad_signals, product_id
                FROM ad_impressions
                WHERE url IS NOT NULL AND url != ''
                """
            )
            prod_df = load_sqlite_table_as_df(
                """
                SELECT id AS product_id, domain AS product_domain, title AS product_title, handle AS product_handle, image_url AS product_image_url, first_seen_at
                FROM products
                """
            )
            if ad_df is None or ad_df.empty:
                st.info("SQLite ä¸­æš‚æ—  ad_impressions æ•°æ®ã€‚è¯·å…ˆæŠ“å–ä¸€è½®å¹¿å‘Šã€‚")
            else:
                x = ad_df.merge(prod_df, on="product_id", how="left")
                if selected_domain:
                    x = x[x["domain"] == selected_domain]
                x["observed_at"] = pd.to_datetime(x["observed_at"], errors="coerce")
                x["landing_type"] = x["url"].astype(str).apply(classify_landing_page)

                t1, t2, t3, t4 = st.tabs([
                    "1) è´¦æˆ·ç»“æž„é€†å‘",
                    "2) è½åœ°é¡µæ‰¿æŽ¥ç­–ç•¥",
                    "3) Hero Product Matrix",
                    "4) Keyword-Product Match",
                ])

                with t1:
                    st.caption("æŒ‰ gad_campaignid èšåˆï¼šsku_count=ä¸åŒè½åœ°é¡µæ•°ï¼Œkeyword_count=æŠ•æ”¾è¯æ•°ï¼Œå¹¶æŒ‰è§„åˆ™æ ‡æ³¨ç­–ç•¥ã€‚")
                    g = (
                        x.groupby("gad_campaignid", dropna=False)
                        .agg(
                            sku_count=("url", "nunique"),
                            keyword_count=("keyword", "nunique"),
                            impressions=("url", "size"),
                        )
                        .reset_index()
                    )

                    def _label_strategy(row):
                        try:
                            sku_count = int(row.get("sku_count") or 0)
                            keyword_count = int(row.get("keyword_count") or 0)
                            if sku_count == 1 and keyword_count < 5:
                                return "å•å“æ‰“çˆ†ç­–ç•¥ (SPAG)"
                            if sku_count > 20:
                                return "é€šæŠ•æµ‹å“ç­–ç•¥ (General)"
                            return "Mixed/Other"
                        except Exception:
                            return "Mixed/Other"

                    g["strategy"] = g.apply(_label_strategy, axis=1)
                    st.dataframe(g.sort_values(["sku_count", "keyword_count"], ascending=False), width="stretch")

                    chart = (
                        alt.Chart(g[g["gad_campaignid"].astype(str).str.len() > 0])
                        .mark_bar()
                        .encode(
                            x=alt.X("gad_campaignid:N", sort="-y", title="Campaign ID (gad_campaignid)"),
                            y=alt.Y("sku_count:Q", title="SKU Count (unique final_url)"),
                            color=alt.Color("strategy:N", title="Strategy"),
                            tooltip=["gad_campaignid", "sku_count", "keyword_count", "impressions", "strategy"],
                        )
                        .properties(height=320)
                    )
                    st.altair_chart(chart, width="stretch")

                with t2:
                    st.caption("æŒ‰è½åœ°é¡µè·¯å¾„ç‰¹å¾åˆ†ç±»ï¼šPDP / Collection / Homeï¼Œå¹¶ç»Ÿè®¡åˆ†å¸ƒã€‚")
                    lp = x.groupby("landing_type").size().reset_index(name="impressions")
                    st.dataframe(lp.sort_values("impressions", ascending=False), width="stretch")
                    pie = (
                        alt.Chart(lp)
                        .mark_arc()
                        .encode(
                            theta=alt.Theta("impressions:Q"),
                            color=alt.Color("landing_type:N", title="Landing Type"),
                            tooltip=["landing_type", "impressions"],
                        )
                        .properties(height=320)
                    )
                    st.altair_chart(pie, width="stretch")

                with t3:
                    st.caption("æŒ‰ product_handle ç»Ÿè®¡å¹¿å‘Šé¢‘æ¬¡ä¸Žå…³é”®è¯è¦†ç›–ï¼Œè¯†åˆ«ä¸»æŽ¨æ¬¾ï¼ˆHero Productï¼‰ã€‚")
                    tmp = x.copy()
                    tmp["product_handle"] = tmp["product_handle"].fillna("")
                    tmp["product_title"] = tmp["product_title"].fillna("")
                    hero = (
                        tmp[tmp["product_handle"].astype(str).str.len() > 0]
                        .groupby("product_handle")
                        .agg(
                            title=("product_title", "first"),
                            image_url=("product_image_url", "first"),
                            ad_frequency=("url", "size"),
                            keyword_count=("keyword", "nunique"),
                            campaign_count=("gad_campaignid", "nunique"),
                            sample_url=("url", "first"),
                        )
                        .reset_index()
                        .sort_values(["ad_frequency", "keyword_count"], ascending=False)
                        .head(10)
                    )

                    # å…³é”®è¯åˆ—è¡¨ï¼ˆTop10 è¡Œæ‰æ‹¼ï¼Œé¿å…çˆ†å†…å­˜ï¼‰
                    kw_map = (
                        tmp[tmp["product_handle"].isin(hero["product_handle"])][["product_handle", "keyword"]]
                        .dropna()
                        .groupby("product_handle")["keyword"]
                        .apply(lambda s: ", ".join(sorted(set([str(v).strip() for v in s if str(v).strip()]))[:50]))
                        .to_dict()
                    )
                    hero["keywords"] = hero["product_handle"].map(kw_map).fillna("")

                    # å¯è¯»å±•ç¤ºï¼šéšè— sku_idï¼Œä»…å±•ç¤º title/handleï¼Œå¹¶æŠŠ handle åšæˆå¯ç‚¹å‡»é“¾æŽ¥
                    hero_show = hero.copy()
                    hero_show["handle_link"] = hero_show["sample_url"]
                    cols = [
                        c
                        for c in [
                            "image_url",
                            "title",
                            "handle_link",
                            "ad_frequency",
                            "keyword_count",
                            "campaign_count",
                            "keywords",
                        ]
                        if c in hero_show.columns
                    ]
                    hero_show = hero_show[cols]
                    col_cfg = {
                        "image_url": st.column_config.ImageColumn("Img", width="small"),
                        "handle_link": st.column_config.LinkColumn("Handle", display_text="æ‰“å¼€"),
                    }
                    st.dataframe(hero_show, width="stretch", column_config=col_cfg)

                with t4:
                    st.caption("åˆ¤æ–­å…³é”®è¯æ˜¯å¦å‡ºçŽ°åœ¨æ ‡é¢˜ä¸­ï¼Œç”¨äºŽè¯†åˆ«â€˜ä¹±æŠ•è¯â€™æœºä¼šç‚¹ã€‚")
                    tmp = x.copy()
                    tmp["keyword_norm"] = tmp["keyword"].astype(str).fillna("").str.lower().str.strip()
                    tmp["title_norm"] = tmp["product_title"].astype(str).fillna("").str.lower()

                    def _match(row):
                        kw = row.get("keyword_norm", "")
                        title = row.get("title_norm", "")
                        if kw and title and kw in title:
                            return "ç²¾å‡†åŒ¹é…"
                        return "å¼±ç›¸å…³"

                    tmp["match_type"] = tmp.apply(_match, axis=1)
                    m = tmp.groupby("match_type").size().reset_index(name="impressions")
                    c1, c2 = st.columns(2)
                    c1.metric("ç²¾å‡†åŒ¹é…", int(m[m["match_type"] == "ç²¾å‡†åŒ¹é…"]["impressions"].sum() if not m.empty else 0))
                    c2.metric("å¼±ç›¸å…³", int(m[m["match_type"] == "å¼±ç›¸å…³"]["impressions"].sum() if not m.empty else 0))
                    st.dataframe(m, width="stretch")

                    weak = (
                        tmp[tmp["match_type"] == "å¼±ç›¸å…³"][
                            ["observed_at", "keyword", "product_title", "gad_campaignid", "url"]
                        ]
                        .sort_values("observed_at", ascending=False)
                        .head(50)
                    )
                    col_cfg = {}
                    if "url" in weak.columns:
                        col_cfg["url"] = st.column_config.LinkColumn("URL", display_text="æ‰“å¼€")
                    st.dataframe(weak, width="stretch", column_config=col_cfg)
        except Exception as e:
            st.error(f"Ad Strategy Decoder åŠ è½½/åˆ†æžå¤±è´¥ï¼š{e}")

    with tab_diff:
        from data_diff import batch_diff
        snapshots = st.session_state.current_session.get("data_snapshots", [])
        prev_df = st.session_state.get("prev_run_df")
        with st.expander("âš–ï¸ æ•°æ®å…¨æ™¯å¯¹æ¯” (Batch Diff)", expanded=True):
            # æ—¶åºå¯¹æ¯”å¼•æ“Žï¼ˆæ¨¡å—ä¸‰ï¼‰
            if prev_df is not None and not df.empty:
                st.subheader("ðŸ“Š ä¸Žä¸Šä¸€è½®æŠ“å–å¯¹æ¯”ï¼ˆåŸºäºŽ sku_idï¼‰")
                prev_df2 = ensure_domain_and_product_id(enrich_dataframe_for_ui(prev_df.copy(), ""))
                curr_df2 = df.copy()
                # å…¨å±€åŸŸåè¿‡æ»¤
                if st.session_state.get("selected_domain"):
                    prev_df2 = prev_df2[prev_df2["domain"] == st.session_state.selected_domain]
                    curr_df2 = curr_df2[curr_df2["domain"] == st.session_state.selected_domain]

                diff_result = batch_diff(prev_df2, curr_df2)
                summary = diff_result["summary"]
                c1, c2, c3, c4 = st.columns(4)
                prev_count = int(summary.get("prev_count", 0) or 0)
                curr_count = int(summary.get("curr_count", 0) or 0)
                delta = summary.get("delta", None)
                try:
                    delta_text = f"{int(delta):+d}" if delta is not None else None
                except Exception:
                    delta_text = None
                c1.metric("ä¸Šä¸€è½® SKU", prev_count, None)
                c2.metric("æœ¬è½® SKU", curr_count, delta_text)
                c3.metric("ðŸ†• æ–°å¢ž", int(summary.get("new_count", 0) or 0), None)
                c4.metric("ðŸ—‘ï¸ ç§»é™¤", int(summary.get("removed_count", 0) or 0), None)

                # å¯è¯» Diff è¡¨
                items = diff_result.get("items", [])
                if items:
                    diff_df = pd.DataFrame(items)
                    # ä½¿ç”¨ canonical_url è®¡ç®— product_idï¼Œå¹¶ join products å˜ä¸ºå¯è¯»æ•°æ®
                    try:
                        diff_df["product_id"] = diff_df["canonical_url"].astype(str).apply(_url_md5)
                    except Exception:
                        diff_df["product_id"] = ""

                    prod_map = sql_df(
                        """
                        SELECT id AS product_id, title AS product_title, handle AS product_handle, image_url AS product_image_url
                        FROM products
                        WHERE domain = ?
                        """,
                        params=(st.session_state.get("selected_domain") or "",),
                    )
                    if prod_map is not None and not prod_map.empty:
                        diff_df = diff_df.merge(prod_map, on="product_id", how="left")
                    if "product_title" not in diff_df.columns:
                        diff_df["product_title"] = ""
                    diff_df["product_title"] = diff_df["product_title"].fillna("").astype(str)
                    diff_df["title_short"] = diff_df["product_title"].apply(lambda s: (s[:50] + "â€¦") if len(s) > 50 else s)
                    diff_df["full_title"] = diff_df["product_title"]
                    diff_df["open_link"] = diff_df.get("canonical_url", "")

                    # join æ ‡é¢˜/ä»·æ ¼ï¼ˆæ¥è‡ª prev/currï¼‰
                    prev_map = prev_df2.set_index("sku_id") if "sku_id" in prev_df2.columns else None
                    curr_map = curr_df2.set_index("sku_id") if "sku_id" in curr_df2.columns else None

                    def _safe_get(m, key, col):
                        try:
                            if m is not None and key in m.index and col in m.columns:
                                v = m.loc[key, col]
                                if isinstance(v, pd.Series):
                                    v = v.dropna().iloc[0] if not v.dropna().empty else None
                                return v
                        except Exception:
                            pass
                        return None

                    def _has_text(v) -> bool:
                        if v is None:
                            return False
                        try:
                            if pd.isna(v):
                                return False
                        except Exception:
                            pass
                        return str(v).strip() != ""

                    rows = []
                    for it in items:
                        sku = it.get("sku_id")
                        status = it.get("status")
                        title_new = _safe_get(curr_map, sku, "Product")
                        title_old = _safe_get(prev_map, sku, "Product")
                        price_new = _safe_get(curr_map, sku, "Price")
                        price_old = _safe_get(prev_map, sku, "Price")

                        title = title_new if _has_text(title_new) else title_old
                        if status == "new":
                            rows.append({"Status": "New", "Title": title, "New Price": price_new})
                        elif status == "modified":
                            try:
                                pct = None
                                if price_old not in [None, ""] and float(price_old) != 0 and price_new not in [None, ""]:
                                    pct = (float(price_new) - float(price_old)) / float(price_old) * 100
                            except Exception:
                                pct = None
                            rows.append({"Status": "Modified", "Title": title, "Old Price": price_old, "New Price": price_new, "Change %": pct})
                    if rows:
                        show = pd.DataFrame(rows)
                        st.dataframe(show, width="stretch")

                    # ä¸»å¯è¯» Diff è¡¨ï¼šå›¾ç‰‡/æ ‡é¢˜/é“¾æŽ¥/çŠ¶æ€/ä»·æ ¼å˜åŒ–ï¼ˆéšè— sku_idï¼‰
                    st.subheader("ðŸ“‹ Diff æ˜Žç»†ï¼ˆå¯è¯»ç‰ˆï¼‰")
                    show2 = diff_df.copy()
                    # å°†ä»·æ ¼å˜åŒ–åˆ—æ‹‰å¹³
                    show2["Status"] = show2["status"].map({"new": "New", "removed": "Removed", "modified": "Modified", "unchanged": "Unchanged"}).fillna(show2["status"])
                    show2["Price Î”"] = show2.get("price_change")
                    show2["Change %"] = show2.get("price_change_pct")
                    display_cols = [
                        c
                        for c in [
                            "product_image_url",
                            "title_short",
                            "full_title",
                            "open_link",
                            "Status",
                            "Price Î”",
                            "Change %",
                        ]
                        if c in show2.columns
                    ]
                    col_cfg = {}
                    if "open_link" in show2.columns:
                        col_cfg["open_link"] = st.column_config.LinkColumn("æ‰“å¼€", display_text="æ‰“å¼€")
                    if "product_image_url" in show2.columns:
                        col_cfg["product_image_url"] = st.column_config.ImageColumn("å›¾ç‰‡", width="small")
                    st.dataframe(show2[display_cols], width="stretch", column_config=col_cfg)
                c4.metric("ðŸ”„ å˜æ›´", summary["modified"], None)
                st.divider()
                # ä»·æ ¼å˜åŒ–æ±‡æ€»
                c5, c6, c7 = st.columns(3)
                c5.metric("ðŸ’° é™ä»·", summary["price_down"], None)
                c6.metric("ðŸ“ˆ æ¶¨ä»·", summary["price_up"], None)
                c7.metric("â­ è¯„è®ºå¢žé•¿", summary["review_growing"], None)
                # è¯¦ç»† diff è¡¨æ ¼ï¼ˆé«˜äº®ï¼‰
                if items:
                    diff_df_display = pd.DataFrame(items)
                    # æ·»åŠ ç®­å¤´åˆ—
                    def price_arrow(row):
                        if row["price_change"] is None:
                            return "â€”"
                        if row["price_change"] < 0:
                            return f"ðŸŸ¢â†“{abs(row['price_change']):.2f}"
                        elif row["price_change"] > 0:
                            return f"ðŸ”´â†‘{row['price_change']:.2f}"
                        else:
                            return "â€”"
                    diff_df_display["ä»·æ ¼å˜åŒ–"] = diff_df_display.apply(price_arrow, axis=1)
                    # å±•ç¤ºåˆ—
                    with st.expander("è°ƒè¯•ï¼šåŽŸå§‹ diffï¼ˆå« sku_idï¼‰", expanded=False):
                        display_cols = ["sku_id", "status", "ä»·æ ¼å˜åŒ–", "domain", "canonical_url"]
                        col_cfg3 = {}
                        if "canonical_url" in diff_df_display.columns:
                            col_cfg3["canonical_url"] = st.column_config.LinkColumn("URL", display_text="æ‰“å¼€")
                        st.dataframe(diff_df_display[display_cols].head(50), width="stretch", column_config=col_cfg3)
                    # SKU è¶‹åŠ¿å›¾å…¥å£ï¼ˆæ¨¡å—å››ï¼‰
                    st.caption("ðŸ’¡ ç‚¹å‡»ä¸Šæ–¹ sku_id å¯æŸ¥çœ‹åŽ†å²è¶‹åŠ¿ï¼ˆåŽç»­æ‰©å±•ï¼‰")
                else:
                    st.info("æ— å˜åŒ–æ•°æ®ã€‚")
                st.divider()
            # å…¼å®¹æ—§ç‰ˆå¿«ç…§ diffï¼ˆä¿ç•™ï¼‰
            if len(snapshots) >= 2:
                st.subheader("ðŸ“‚ ä¼šè¯å†…å¿«ç…§ Diffï¼ˆæ—§ç‰ˆå…¼å®¹ï¼‰")
                prev_snap = snapshots[-2]
                curr_snap = snapshots[-1]
                prev_df_snap = prev_snap.get("df") if isinstance(prev_snap, dict) else None
                curr_df_snap = curr_snap.get("df") if isinstance(curr_snap, dict) else None
                if prev_df_snap is not None and curr_df_snap is not None:
                    diff_result_old = compute_diff(prev_df_snap, curr_df_snap)
                    summary_old = get_diff_summary_for_ui(diff_result_old)
                    c1, c2, c3, c4 = st.columns(4)
                    c1.metric("ä¸Šä¸€å¿«ç…§ è¡Œæ•°", summary_old["row_count_prev"], None)
                    c2.metric("å½“å‰å¿«ç…§ è¡Œæ•°", summary_old["row_count_curr"], f"{summary_old['row_delta']:+d}")
                    c3.metric("ðŸ“ˆ æ–°å¢ž", summary_old["added_count"], None)
                    c4.metric("ðŸ“‰ ç§»é™¤", summary_old["removed_count"], None)
                    for col, vals in summary_old.get("numeric_deltas", {}).items():
                        st.metric(f"æ•°å€¼åˆ—ã€Œ{col}ã€å‡å€¼", vals["curr"], f"{vals['delta']:+.2f}")
                    with st.expander("æŸ¥çœ‹è¯¦ç»†å˜æ›´ (æ–°å¢ž/ç§»é™¤çš„ ID æˆ–è¡Œ)"):
                        st.write("**æ–°å¢ž:** ", summary_old["added_ids"][:50])
                        st.write("**ç§»é™¤:** ", summary_old["removed_ids"][:50])
                else:
                    st.caption("ä»…ä¿ç•™å¿«ç…§å…ƒæ•°æ®ï¼Œæ— å®Œæ•´ DataFrameï¼Œæ— æ³•è®¡ç®—è¯¦ç»† Diffã€‚ä¸‹æ¬¡ç”ŸæˆæŠ¥å‘ŠåŽå°†ä¿ç•™å¿«ç…§ç”¨äºŽå¯¹æ¯”ã€‚")
            else:
                st.info("ðŸ‘ˆ æš‚æ— åŽ†å²æ•°æ®å¯ä¾›å¯¹æ¯”ã€‚è¯·å…ˆæŠ“å–ä¸€æ¬¡æ•°æ®ï¼Œå†æŠ“å–ç¬¬äºŒæ¬¡å³å¯çœ‹åˆ° Batch Diffã€‚")

        with st.expander("ðŸ”Ž å•å“æ·±åº¦åˆ†æž", expanded=False):
            try:
                selected_domain = st.session_state.get("selected_domain")
                if selected_domain:
                    drill = load_sqlite_table_as_df(
                        """
                        SELECT ai.product_id, p.title AS product_title, p.handle AS product_handle, p.image_url AS product_image_url,
                               COUNT(*) AS ad_frequency
                        FROM ad_impressions ai
                        LEFT JOIN products p ON p.id = ai.product_id
                        WHERE ai.domain = ?
                        GROUP BY ai.product_id
                        ORDER BY ad_frequency DESC
                        LIMIT 20
                        """.replace("?", f"'{selected_domain}'")
                    )
                    if drill is None or drill.empty:
                        st.caption("å½“å‰åŸŸåä¸‹æš‚æ— å¯ç”¨äºŽæ·±åº¦åˆ†æžçš„å•†å“ï¼ˆè¯·å…ˆæŠ“å–å¹¶å…¥åº“ï¼‰ã€‚")
                    else:
                        drill["label"] = drill.apply(
                            lambda r: f"{r.get('product_title','') or r.get('product_handle','')} ({r.get('ad_frequency',0)})",
                            axis=1,
                        )
                        sel = st.selectbox("é€‰æ‹©å•†å“ï¼ˆTop 20 by Ad Frequencyï¼‰", options=drill["label"].tolist(), index=0)
                        product_id = drill.loc[drill["label"] == sel, "product_id"].iloc[0]

                        states = load_sqlite_table_as_df(
                            f"SELECT observed_at, price FROM product_states WHERE product_id = '{product_id}' ORDER BY observed_at"
                        )
                        imps = load_sqlite_table_as_df(
                            f"SELECT observed_at FROM ad_impressions WHERE product_id = '{product_id}' ORDER BY observed_at"
                        )

                        if states is not None and not states.empty:
                            states["observed_at"] = pd.to_datetime(states["observed_at"], errors="coerce")
                            chart1 = (
                                alt.Chart(states.dropna(subset=["observed_at"]))
                                .mark_line(point=True)
                                .encode(
                                    x=alt.X("observed_at:T", title="æ—¶é—´"),
                                    y=alt.Y("price:Q", title="Price"),
                                    tooltip=[
                                        alt.Tooltip("observed_at:T", title="Time"),
                                        alt.Tooltip("price:Q", title="Price"),
                                    ],
                                )
                                .properties(height=260, title="ä»·æ ¼èµ°åŠ¿")
                            )
                            st.altair_chart(chart1, width="stretch")
                        else:
                            st.caption("æš‚æ— ä»·æ ¼çŠ¶æ€è®°å½•")

                        if imps is not None and not imps.empty:
                            imps["observed_at"] = pd.to_datetime(imps["observed_at"], errors="coerce")
                            freq = imps.groupby(pd.Grouper(key="observed_at", freq="D")).size().reset_index(name="impressions")
                            chart2 = (
                                alt.Chart(freq.dropna(subset=["observed_at"]))
                                .mark_line(point=True)
                                .encode(
                                    x=alt.X("observed_at:T", title="æ—¶é—´"),
                                    y=alt.Y("impressions:Q", title="Ad Impressions (Daily)"),
                                    tooltip=[
                                        alt.Tooltip("observed_at:T", title="Date"),
                                        alt.Tooltip("impressions:Q", title="Impressions"),
                                    ],
                                )
                                .properties(height=260, title="å¹¿å‘ŠæŠ•æ”¾åŠ›åº¦ï¼ˆæŒ‰å¤©ï¼‰")
                            )
                            st.altair_chart(chart2, width="stretch")
                        else:
                            st.caption("æš‚æ— å¹¿å‘Šæ›å…‰è®°å½•")
                else:
                    st.caption("è¯·å…ˆåœ¨å·¦ä¾§é€‰æ‹©ç«žäº‰å¯¹æ‰‹åŸŸåã€‚")
            except Exception as e:
                st.error(f"å•å“æ·±åº¦åˆ†æžåŠ è½½å¤±è´¥ï¼š{e}")

else:
    st.divider()
    st.header("ðŸ“Š æ€»è§ˆ")
    st.info("å½“å‰è¿˜æ²¡æœ‰å¯åˆ†æžçš„æ•°æ®ã€‚ä½ å¯ä»¥åœ¨å·¦ä¾§å…ˆã€ŽðŸ“‚ åŠ è½½æ•°æ®ã€æˆ–å¯åŠ¨æŠ“å–å¼•æ“Žç”Ÿæˆæ•°æ®ã€‚")
    tab_report, tab_ai, tab_adgroup, tab_decoder, tab_diff = st.tabs([
        "ðŸ“„ æ·±åº¦æŠ¥å‘Š",
        "ðŸ’¬ AI æ•°æ®é¡¾é—®",
        "ðŸ“ å¹¿å‘Šç»„è§†å›¾",
        "ðŸ§© æŠ•æ”¾ç­–ç•¥è§£å¯† (Ad Strategy Decoder)",
        "âš–ï¸ Batch Diff",
    ])
    with tab_report:
        st.caption("è¯·å…ˆåŠ è½½åŽ†å²æ•°æ®æˆ–å®Œæˆä¸€æ¬¡æŠ“å–åŽå†ç”ŸæˆæŠ¥å‘Šã€‚")
    with tab_ai:
        st.caption("è¯·å…ˆåŠ è½½åŽ†å²æ•°æ®æˆ–å®Œæˆä¸€æ¬¡æŠ“å–åŽå†è¿›è¡Œæ•°æ®é—®ç­”ã€‚")
    with tab_adgroup:
        st.caption("è¯·å…ˆåŠ è½½åŽ†å²æ•°æ®æˆ–å®Œæˆä¸€æ¬¡æŠ“å–åŽå†æŸ¥çœ‹å¹¿å‘Šç»„è§†å›¾ã€‚")
    with tab_decoder:
        st.caption("è¯·å…ˆæŠ“å–å¹¶å…¥åº“åŽå†è¿›è¡ŒæŠ•æ”¾ç­–ç•¥è§£å¯†åˆ†æžã€‚")
    with tab_diff:
        st.caption("è¯·å…ˆå®Œæˆä¸¤è½®æŠ“å–ï¼ˆæˆ–åŠ è½½åŒ…å«ä¸Šä¸€è½®æ•°æ®çš„åŽ†å²è®°å½•ï¼‰åŽå†æŸ¥çœ‹ Batch Diffã€‚")
        with st.expander("ðŸ”Ž å•å“æ·±åº¦åˆ†æž", expanded=False):
            st.caption("å…¥å£å·²æ”¶æ•›ä¸ºä»…åœ¨ã€Žâš–ï¸ Batch Diffã€é‡Œå±•ç¤ºã€‚åŠ è½½/æŠ“å–æ•°æ®åŽå¯ç”¨ã€‚")
